import './styles.css'

import { Head, Image, Appear, Notes, Invert, Split } from "mdx-deck"

import { CodeSurfer, CodeSurferColumns, Step } from "code-surfer"
import { shadesOfPurple } from "@code-surfer/themes"
import theme from './theme'

import thanos from './static/thanos_logo.svg'
import prometheus from './static/prometheus_logo.svg'
import redhat from './static/red_hat_logo.png'

export const themes = [
	shadesOfPurple,
	theme,
];

<Head>
	<title>Are you testing your Observability?</title>
</Head>

<CodeSurfer>

```go
	talks_total{title="    Are you testing your Observability?    "} 2
	talks_total{subtitle="       --- Metrics Edition ---       "} 2



	talks_total{conference="FOSDEM", when="02.02.2020", where="Brussel"} 1
```

</CodeSurfer>

<Notes>

Hello everyone!

We are extremely excited to be here in FOSDEM conference, and be able to speak about the topic we love Observability.

We hope our talk will be very inspiring and actionable for you.
This is because at the end of this talk we would like you to know 3 THINGS:

* Why instrumenting backend applications with actionable metrics is essential
* How to instrument your service quickly for Prometheus metric system to use
* And last but not the least: What are the common mistakes you should avoid, mistakes that
we seen a lot during our work with Go and metrics in the amazing (but sometimes WILD) OPEN SOURCE WORLD.

But before that: Short introduction!

</Notes>

---

import bartek from './static/bartek.jpeg'
import kemal from './static/kemal.jpeg'
import twitter from './static/twitter.png'
import github from './static/github.png'

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={bartek} style="height: 90%; float: left; margin-top: 20px; padding: 0 20px 0 20px"/>

#### Bartek Plotka

<div style="font-size: 50%">
Principal Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Prometheus and Thanos Maintainer<br/><br/>

<img src={twitter} style="height: 40px; width: 40px;"/> <img src={github} style="height: 40px; width: 40px;"/> @bwplotka
</div>
</div>

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={kemal} style="height: 90%; float: left; margin-top: 20px; padding: 0 20px 0 20px"/>

#### Kemal Akkoyun

<div style="font-size: 50%">
Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Thanos Contributor<br/><br/>

<img src={twitter} style="height:40px; width: 40px;"/> @kkakkoyun <span/>
<img src={github} style="height: 40px; width: 40px;"/> @kakkoyun
</div>
</div>

<Notes>

My name is Bartek Plotka, I am an engineer working at Red Hat in the Monitoring team, I love open source and solving problems
using Go.
I am part of Prometheus Team and I am a co-author of Thanos project, which is a durable system for scaling Prometheus.

With me there is Kemal...

Hello everyone, my name is Kemal. I am also a software engineer in the OpenShift Monitoring team, at Red Hat.
I love everything related to Go, Prometheus, Kubernetes, and I try to contribute to those projects as much as I can.

</Notes>

---

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={prometheus} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
</div>

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={thanos} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
</div>

<Notes>

Our job is focused on building scalable Observability solutions and platforms for OpenShift.
But also as one of the major part of our work is maintaining Prometheus and Thanos projects on a daily basis.
Those projects are focused on enabling monitoring via metrics for infrastructure, server side applications e.g
in microservices running on Kubernetes. While we work on enabling metrics from backend side, we also
are part of the client side of metrics, so applications that are being monitored. And this is the part we will be focusing
on today.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Let's implement an HTTP L7 loadbalancer! ❤</h2>

---

import d01 from './static/d01.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Because why not?</h2>

<Appear>
	<Image src={d01} size='contain'/>
</Appear>

<Notes>

Today we have a fun task! We will talk today about building loadbalancer. Kind of.

For demo purposes let's imagine we want to implement application level HTTP loadbalancer. Let's say in Go, but
programming language does not matter here.

1) So let's say we implemented transparent loadbalancer as presented in this diagram.
From high level design we have couple of Go components.

- Single HTTP server that implements ServeHTTP method, so handler via awesome ReverseProxy in standard httputil package.
- that ReverseProxy allows us to inject custom Transport, so RoundTripper interface.
- We inject there our load balancing RoundTipper implementation called lbtranport, which is internally
using then few components:
-- Discoverer which gives us targets to proxy / loadbalance request to
-- RoundRobinPicker which chooses right target to proxy user request to in FAIR, round robin manner, so: replica 1, 2,3, then again 1, 2, 3
-- And at the end it uses http.Transport to forward request to picked replica and proxy response back to the user.

</Notes>

---

import d02 from './static/d02.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d02} size='contain'/>

<Notes>

This is great, it looks like this implementation should work.. but are we sure it's production ready?

So let's say we deploy couple of replicas of our loadbalancer in production in front of some microservice and let
it run for longer time.

As soon as it starts running, we hit /lb endpoint manually and we can see it works. So we are good, right?

Well...

</Notes>

---

import d03 from './static/d03.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d03} size='contain'/>

<Notes>

Not necessarily. It works for me but are we sure loadbalancer works as expected all the time?
Does it actually work for all the users or only me? How many Bad Gateway Errors it was returning over time?
We can't really tell!

</Notes>

---

import d05 from './static/d05.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d05} size='contain'/>

<Notes>

Is round robin picker, picking in round robin matter?
Is 1/3 of all requests actually going to replica 2? What's was the distribution of request over time? Was it fair?

</Notes>

---

import d06 from './static/d06.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d06} size='contain'/>

<Notes>

What if users reports that the endpoint is slow: is it the backend that is slow?
Or is loadbalancing logic that is introducing the latency?

</Notes>

---

import d08 from './static/d08.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d08} size='contain'/>

<Notes>

Finally what version of the loadbalancer we were running 2 days ago at 2pm? Maybe something was wrong back then and we
are not sure what version was actually rolled on Kubernetes...

</Notes>

---

import d09 from './static/d09.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> We need Monitoring!</h2>

<Image src={d09} size='contain'/>


<Notes>

As you can see there are massive amount of questions that would be not answered when running the service like this on
production, without proper monitoring.

That's why in SRE book you will find monitoring as the foundation of any system, BEFORE even the system itself!

As you might be familiar, some monitoring signals we can introduce are: traces, logs and metrics.
Guess which signal will give us answer to our questions like distribution of requests or histogram of tail latency?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Let's instrument our LB with Prometheus metrics!</h2>

<ul style="font-size: 80%; margin-top: 95px">
<Appear>
<li>Cheap</li>
<li>Near Real Time</li>
<li>Actionable (Alert-able)</li>
<div>
    <img src={prometheus} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
    <p>http://prometheus.io</p>
</div>
</Appear>
</ul>

<Notes>

Metrics, yup!

Metrics most likely give us the answers to our questions.
Answer that is in comparison to logs and traces:

1) CHEAPer to calculate
Near Real Time
Clear and Actionable, so you can alert on those.

In practice metrics should be the first item on monitoring list that you should do if you care to run your service reliably.

2) Why Prometheus though? Well I might be bias but Prometheus is currently one of the simplest and
cheapest option for collecting, storing and querying metrics as well as reliable alerting.
It is part of CNCF, fits for small solution as well as for bigger ones with help of cloud native projects like Thanos, Cortex, m3db and others

</Notes>

---

import d11 from './static/d11.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<Image src={d03} size='contain'/>

<Notes>

So.. how to add Prometheus metrics to our loadbalancer?
Let's say we want to answer for how many users our loadbalancer is actually responding working, and for how many
it returns error!

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<Image src={d11} size='contain'/>

<Notes>

We can do that by incrementing some certain http_requests_total counter whenever a loadbalancing request occur
reporting method that was used, and HTTP status code that was returned.

We can introduce this metric really in few simple steps.

</Notes>

---

<CodeSurfer>

```go title="Server HTTP request counter" subtitle="Import Prometheus Go client package."

# You can find list of all available clients for 18 programming languages here:
# https://prometheus.io/docs/instrumenting/clientlibs/#client-libraries
import "github.com/prometheus/client_golang/prometheus"

```

```go 5,6,7,8,9,10 title="Server HTTP request counter" subtitle="Define counter."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

```

```go title="Server HTTP request counter" subtitle="Instrument ServeHTTP function."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

```go 23 title="Server HTTP request counter" subtitle="Register our metric."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

```

```go title="Server HTTP request counter" subtitle="Add /metrics handler."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

mux := http.NewServeMux()
mux.Handle("/metrics", promhttp.Handler())
mux.Handle("/lb", ...)

// Other handlers...

srv := &http.Server{Handler: mux}

// Run server...

```

</CodeSurfer>

<Notes>

1) First of all we have to import Prometheus Go client library. We will show here an
example on how to add metric in Go language, but it's equally simple in other languages as well.
Similar libraries exists in 18 other programming languages.

However, we are focusing here on Go, since we are working with Go already 5-6years.

2) To add server HTTP metrics, As a next step we need to define variable for the our counter of requests. We pick a name, a description
and certain "labels" which will be our dimensions for this counter. Each unique value in any of those labels will
result in totally new series in Prometheus system.

3) Next step is to actually count our requests. Let's create a simple wrapper of http server which will increment
the counter with the status code that the server returned and requested method.

4) Something that is easy to forget is another step: The counter has to be registered somewhere in order
to be exposed for Prometheus. Let's do this once in `init` function and register it in GLOBAL
Prometheus registry.

Let's focus on what we accomplish by registering this metric. It's important for the next step which is

5) HTTP handler for metric page. It serves simple text page with all metrics.
Once we add that to our loadbalancer, our server  is correctly exposing the http_requests_total counter
we created to the outside world.

</Notes>

---

import graph_requests from './static/graph-requests.png'

<CodeSurfer>

```go 4,6 title="From code to graph" subtitle="We defined & instrumented our metric."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

```

```go 4,6,10,13,14 title="From code to graph" subtitle="It is now exposed under /metrics."
var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Part of response from /metrics HTTP endpoint.
# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="get"} 1089
http_requests_total{code="500", method="get"} 46

```

</CodeSurfer>

<Notes>

1) How we can now use our metric? As you remember we defined this metric like this and we increment it every HTTP request.
2) Loadbalancer now serves /metrics page which exposes our metrics in Prometheus supported text format.
As you can see each code and method are a separate counter, with mostly successes.

</Notes>

---

import d10 from './static/d10.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Prometheus can now collect metrics from our Loadbalancer</h2>

*Pulling e.g every 15 seconds*

<Image src={d12} size='contain'/>

<Notes>

We now can use running binary of Prometheus and point to the loadbalancer /metrics page. Prometheus then will
visit this page (which is called scrape) every given interval and collect all exposed metrics.

</Notes>

---

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Graph: Prometheus UI</h2>

<img src={graph_requests} style="height: 70%; margin-top: 5%"/>

<Notes>

With that we can after some time, visit Prometheus UI where we can produce graphs. For example we can query
the number of requests per minute by code and method. We can see that per minute we have 120 requests in total,
with some small portion of those being error responses.

</Notes>

---

import pitfalls from './static/pitfall.gif'

<div>
    <h1 style="text-align: center; margin: 0px 10px 0 10px;">
        <img src={prometheus} style="width: 10%; margin-top: 5%"/>
        &nbsp;&nbsp;Pitfalls
    </h1>
</div>

<img src={pitfalls} style="height: 50%; margin-top: 5%"/>

<Notes>

So it looked easy right? And it is easy in most cases. However during this talk we would like
to present what we learnt during couple of years of developing and reviewing instrumentation code that
is meant to be run on production, in close but mainly in open source.

Together with Kemal wil go though a few less or more advanced issues we seen and how to resolve them.

</Notes>

---

import magic from './static/magic.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #1: Global Registry</h2>
<br/>
<br/>
<h6 style="text-align: center; margin: 0 10px 0 10px;">"magic is bad; global state is magic" by Peter Bourgon</h6>
<br/>
<img src={magic} style="height: 50%; margin-top: 5%"/>

<Notes>

First one! Globals.

There was a saying in amazing Peter Bourgon blog post "A theory of Modern Go": magic is bad; global state is magic.

This is very true also in case of Prometheus client, especially if you are instrumenting some library with metrics
that your project, or maybe anyone in open source is using. This Prometheus library especially Go one,
allows you to use globals for certain simplicity, however the usage of it leaked as a pattern which we really really want to obsolete.

So let's focus on where you can have magic in our metrics and what can go wrong, on example of our loadbalancer service.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #1: Global Registry" subtitle="We have 2 global variables here."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```diff 3,4,5,6,7,8 title="Pitfall #1: Global Registry" subtitle="Package-level metric variable."
```

```go 12[16:80] title="Pitfall #1: Global Registry" subtitle="and global DefaultRegisterer."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Global Registry" subtitle="What if another package will create metric with same name?"

// Inside package A:
var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

// Somewhere inside package X imported by your dependency:
func init() {
    // PANIC!
    // You cannot register same metric name twice.
    prometheus.MustRegister(prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Tracks the number of different HTTP requests.",
        }, []string{"code", "method"},
    ))
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go 15,16,17,18,19,20,21,22 title="Pitfall #1: Globals: No flexibility" subtitle="What if I have more handlers than one?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Globals: No flexibility" subtitle="Nice, but for what handler?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="get"} 2445
http_requests_total{code="500", method="get"} 53

```

```go title="Pitfall #1: Getting rid of globals"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Introduce instance of metrics!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Create new instance of ServerMetrics."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

metrics := NewServerMetrics()

func init() {
	prometheus.DefaultRegisterer.MustRegister(metrics.requestsTotal)
}

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Register using Custom Registerer (composition!)"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()
metrics := NewServerMetrics(reg)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

``` go title="Pitfall #1: Getting rid of globals" subtitle="Is it ok now?"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

// This will panic, because we register same metric 3 times...
metrics1 := NewServerMetrics(reg)
metrics2 := NewServerMetrics(reg)
metrics3 := NewServerMetrics(reg)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go 23,26,29 title="Pitfall #1: Getting rid of globals" subtitle="We can wrap register to inject label."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
)
metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
)
metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="We can have request counter per handler (:"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
)
metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
)
metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
)

// For endpoint /one:
metrics1.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics2.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics3.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{handler="/one", code="200", method="get"} 1445
http_requests_total{handler="/one", code="500", method="get"} 23
http_requests_total{handler="/two", code="200", method="get"} 445
http_requests_total{handler="/two", code="500", method="get"} 0
http_requests_total{handler="/three", code="200", method="get"} 645
http_requests_total{handler="/three", code="500", method="get"} 40

```

</CodeSurfer>

<Notes>

1) Let's take our example or http requests_total metric. We have 2 global states here.

2) As you can see metric is a global, package level variable. We register it once per package import as well.

3) Second place of global state is MustRegister which is actually hiding a Global DefaultRegisterer, so
we are registering out metric in global state of Prometheus library.

Now what's the issue here? Why is that problematic?

4) First problem is well magic.. If another package you just import, or you dependency imports registers a metrics
with the same name your application will crash at start.
Even worse the stacktrace will give you the clue where the second register happened but nothing about first one!
We have seen a lot of those problems, so please don't use globals (:

5) Second issue is lack of flexibility! What if you have more than one handler, more than one endpoint?

6) As you can see the insight you gain is pretty limited as the requests are counted per all endpoints. I can't tell
what's the error rate for /three endpoint for example.

Let's try to fix this!

7) And by fixing I mean removing globals!

8) Let's replace global variable with some struct that you can instantiate. It will have constructor that will create
our counter.

9) Now you can create such object and use it everywhere.

10) To eliminate last global behind registering, let's inject custom register into our constructor and instantiate
our own registry which we control in explicit way during our application lifetime!

11) Now what if we try to have different metrics for each handler? It will panic again, but there is nice solution
to that and since we are in control let's apply it.

12) Prometheus allows wrapping registry with custom prefix or labels, so we can inject handler label for each endpoint.

13) Now we have our metrics nicely grouped by handler as well, so we have additional crucial insight on what's
the number of requests per endpoint as well.

</Notes>

---

import testing from './static/testing.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #2: No Tests For Metrics</h2>
<br/>
<br/>
<img src={testing} style="height: 50%; margin-top: 5%"/>

<Notes>

This is something I am really passionated about.
Metrics and other observability signals like e.g tracing, logs and profiles are rarely tested.

Who is asserting in unit test if log or trace was produced for certain even and if it has certain message.
It's not always true but logs are usually used by humans so there is no point in checking exact message.

With metrics in my opinion it's totally different story. They has to be tested. Let me explain in second why.

</Notes>

---

import d12 from './static/d12.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's test our LB!</h2>

<Image src={d11} size='contain'/>

<Notes>

So let's take our loadbalancer, again and newly added HTTP request total counter.
We are solid 10x developers right, so we want to test our code, so we wrote
some unit test!

</Notes>

---

import d13 from './static/d13.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Unit testing lbtransport.RoundTripper</h2>

<Image src={d13} size='contain'/>

<Notes>

And yes, it involves Mocking the Discoverer and Round Robin Picker.
Mocking our Replicas for some case like, for example all replica being down
and not available.

And sending few mocked HTTP requests against that.

</Notes>

---

import d14 from './static/d14.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Unit testing lbtransport.RoundTripper</h2>

<Image src={d14} size='contain'/>

<Notes>

So we run this test, we run 3 request, we assert that response is 502, so Bad Gateway, we could
not loadabalance - expected error case that can happen.

And we are good, we can test different cases in similar way, all passed and we are fine, right?
</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="How it looks in Go code?"

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

}

```

</CodeSurfer>

<Notes>

1) Ok, so let's see how it looks in our Go code! And Let's take our http requests example.
2) We then create a unit test, Send 3 HTTP requests, Assert 3x responses 502 status code.. and all good!

</Notes>

---

import d15 from './static/d15.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Do we verified everything?</h2>

<Image src={d15} size='contain'/>

<Notes>

Nice, but what if did not instrument our metric correctly, right?

For example we made a bug while incrementing a http request total metric, and we always
instrument it with 200 HTTP request code.

Now what you can see.. our unit test passed just fine, however the actual metric exposed
to Prometheus shows incorrect information.

You can think that this is not a big deal, just some analytics will mislead. Well it is actually
serious.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="Alert for too many errors will be missed."

alert: HttpTooMany502Errors
  expr: |
    sum(rate(http_requests_total{code="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with code 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

</CodeSurfer>

<Notes>

Imagine this alert, it's one the...

</Notes>

---

import d16 from './static/d16.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Let's verify correct metric instrumentation!</h2>

<Image src={d16} size='contain'/>

<Notes>

What we need to do, is actually verify more - so check if we instrumented out metric well!

</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric value."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Assert http_requests_total{code="502", method="get"} == 3 .
    testutil.Equals(t, 3,
        promtestutil.ToFloat64(metrics.requestsTotal.WithLabelValues("502", "get")),
     )
}

```

```go title="Pitfall #2: Not testing" subtitle="Such unit test will detect the problem!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    // ...
    lb := NewLoadbalancingTransport(nil, mockedDiscovery, mockedPicker, mockedTransport)
​
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)
    testutil.Equals(t, 502, rec.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Assert http_requests_total{code="502", method="get"} == 3 .
    // === RUN   TestLoadbalancer/#00
    //       --- FAIL: TestLoadbalancer/#00 (0.00s)
    //           transport_test.go:190:
    //
    //               exp: 3
    //
    //               got: 0
    //   FAIL
    //
    testutil.Equals(t, 3,
        promtestutil.ToFloat64(metrics.requestsTotal.WithLabelValues("502", "get")),
     )
}

```

</CodeSurfer>

<Notes>

1) TODO

We really recommend to extend your normal tests with metrics assertion for all your crucial metrics.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #3: Lack of Consistency</h2>
<br/>
<br/>
<Appear>

<h5 style="text-align: center; margin: 0 10px 0 10px;">The Four Golden Signals, USE and RED methods</h5>

<div>
<br/>
<br/>
<ul>
<li><span style="color: red">R</span>: Requests per second (saturation).</li>
<li><span style="color: red">E</span>: Errors per second.</li>
<li><span style="color: red">D</span>: Duration (tail latency).</li>
</ul>
</div>
</Appear>

<Notes>

There are a some useful high-level methods on what metrics you should define, to properly monitor your ONLINE system.

1. The four golden signals from SRE book, USE method, RED method.

	There are certain advantages to follow them:

	- They help to ensure you have main signals upfront, for potential debugging or alerting needs.

	- They help to reuse common alerts, recording rules and dashboards. For example, monitoring-mixins.

2. Let's focus on RED method as an example: R stands for request/sec, E for error, D for the duration.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #3: Lack of consistency" subtitle="Does this satisfy RED method?"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```diff 11,13 title="Pitfall #3: Lack of consistency" subtitle="R = Requests"
```

```diff 13[25:30] title="Pitfall #3: Lack of consistency" subtitle='E = Errors (code = "5..")'
```

```diff 10[1:2] title="Pitfall #3: Lack of consistency" subtitle="D = Duration is Missing!"
```

```go 4,16,17,18,19,20,21,22,23,24,31,32 title="Pitfall #3: Consistency" subtitle="D = Histogram for observing latency"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
        requestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "http_request_duration_seconds",
                Help:    "Tracks the latencies for HTTP requests.",
                Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120},
            },
            []string{"code", "method"},
        ),
	}
	reg.MustRegister(m.requestsTotal, m.requestDuration)
	return ins
}

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	start := time.Now()
	defer metrics.requestDuration.WithLabelValues(statusRec.Status(), r.Method)).Observe(time.Since(start))

	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

</CodeSurfer>

<Notes>

Let's see an example. We have an `http_requests_total` metric to track the number of HTTP requests.

Does our Server Metrics satisfy RED method?

1. It certainly counts number of requests per method and status code.

1. We can extract `Errors` easily by checking status code.

1. But we don't have anything to track request duration!

1. By adding a histogram that observes latency of the requests, we can satisfy RED method.

Now our loadbalancer has consistent metrics and can reuse RED dashboards, alerts and recording rules, which
is just awesome.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #4: Naming - Not conforming naming convention</h2>

<Notes>

**Of course, one of the most common pitfall is about naming...**

...

</Notes>

---

> There are only two hard things in Computer Science: _cache invalidation_ and _naming things_.

> Phil Karlton

<Notes>

Because you know naming things is hard!

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">There is an official documentation on naming conventions</h2>

<div style="font-size: 80%; text-align: center">
	<h6><a>https://prometheus.io/docs/practices/naming/#metric-and-label-naming</a></h6>
</div>

<Notes>

But lucky for us we have an official documentation for naming `metrics`.

**Use it!** It's really helpful.

</Notes>

---

<Notes>

A couple of points to highlight from the official documentation.

1. Names should have a suffix describing the **base** unit in plural form.

1. An _accumulating_ counter has to have *_total* as a suffix, in addition to the unit.
Actually, with new **OpenMetrics** standards this will become mandatory.

1. Put *_info* suffix at the end, for the metrics that provides metadata about your running system.

</Notes>

<CodeSurfer>

```go title="Pitfall #4: Naming - Not conforming naming convention"
```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="Names should have a suffix describing the unit."

http_request_duration_seconds
node_memory_usage_bytes

```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="Counter names have *_total* as a suffix."

http_request_duration_seconds
node_memory_usage_bytes

http_requests_total
process_cpu_seconds_total

```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="_info suffix for metadata metric names."

http_request_duration_seconds
node_memory_usage_bytes

http_requests_total
process_cpu_seconds_total

build_info

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #4: Naming - Stability</h2>

<Notes>

There is one other important aspect of naming: **it's stability**.

</Notes>

---

<Notes>

Let's use the same metric from previous examples, the one that tracks HTTP requests.

1. Let's build an alert using it.

1. At a certain point in time, for whatever reason, you decide to change your metric and you add *_protocol_* in the name.

1. **NOW** This alert will never fire but also will not fail!
**renaming** can cause issues like this, in Alerts, Recording rules, Dashboards and possibly more...

So be consistent, be careful with the names.

</Notes>

<CodeSurfer>

```go title="Pitfall #4: Naming - Stability"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="Pitfall #4: Naming - Stability" subtitle="Let's say we alert on too many 5xx responses."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go 11[29:37] title="Pitfall #4: Naming - Stability" subtitle="Let's say we are renaming metric..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go title="Pitfall #4: Naming - Stability" subtitle="Ooops..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

## BOOM!💥 This alert will never fire but also will not fail!
alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

</CodeSurfer>

---
import harry_potter from './static/harry_potter.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #5: Cardinality - Unbounded labels</h2>

<img src={harry_potter} style="height: 50%; width: 80%; margin-top: 5%"/>

<Notes>

When we talk about Prometheus and **performance**, it always comes down to cardinality.

What is cardinality actually?

In Prometheus context, the cardinality is the amount of unique time-series you have in your system.

And don't forget, each unique label value, that you add to your metric, creates a new time-series.

`Labels what make Prometheus strong`. However you should always watch out how you use them.

Things could get out of control pretty quickly.

Let's see an example.

</Notes>

---

<Notes>

Let's use our usual suspect again.

2. But this time, let's add an additional label to track each requests per *path*.

3. Let's check out our metrics...
It looks reasonable, we just have a couple of addition series.

3. Let's have a closer look! *OOPS*. That is not what we expected.

We have a lot noise and random request from internet.
Internet is not always a nice place.

We shouldn't use arbitrary data as label values, such as the dynamic paths, session ids, request ids.
So, **always keep track of things that you put in your labels.**

If you want to track your discreet events, better to use a logging system.

</Notes>

<CodeSurfer>

```go title="Pitfall #5: Cardinality - Unbounded labels" subtitle="Let's define a metric." 5:9

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="#5: Cardinality - Unbounded labels"  subtitle="Let's track request per path"

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method", "path"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="#5: Cardinality - Unbounded labels" subtitle="Let's check out our metrics..."

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="GET", path="/metrics"} 15
http_requests_total{code="200", method="GET", path="/status"} 2
http_requests_total{code="200", method="GET", path="/ping"} 123
http_requests_total{code="200", method="GET", path="/articles"} 221
http_requests_total{code="200", method="GET", path="/article/1"} 1
http_requests_total{code="200", method="GET", path="/article/2"} 14
http_requests_total{code="200", method="GET", path="/article/3"} 10

```

```go title="#5: Cardinality - Unbounded labels" subtitle="Let's look closer 😲"

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="GET", path="/metrics"} 15
http_requests_total{code="200", method="GET", path="/status"} 2
http_requests_total{code="200", method="GET", path="/ping"} 123
http_requests_total{code="200", method="GET", path="/articles"} 221
http_requests_total{code="200", method="GET", path="/article/1"} 1
http_requests_total{code="200", method="GET", path="/article/2"} 14
http_requests_total{code="200", method="GET", path="/article/3"} 10
http_requests_total{code="401", method="GET", path="/articles"} 221
http_requests_total{code="401", method="GET", path="/article/1"} 1
http_requests_total{code="401", method="GET", path="/article/2"} 14
http_requests_total{code="401", method="GET", path="/article/3"} 10
http_requests_total{code="401", method="GET", path="/article/4"} 1
http_requests_total{code="403", method="GET", path="admin"} 287
http_requests_total{code="404", method="GET", path="/article/112"} 6
http_requests_total{code="404", method="GET", path="/article/222"} 48
http_requests_total{code="404", method="GET", path="/article/hello"} 10
http_requests_total{code="404", method="GET", path="/article/99"} 1
http_requests_total{code="404", method="GET", path="/robots.txt"} 11424
http_requests_total{code="404", method="GET", path="/lookup/zzx"} 12
http_requests_total{code="404", method="GET", path="/helloissomeonethere"} 1

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #6: Cardinality - Histogram Explosion</h2>

<Notes>

Histograms are more complex metric types compare to counters and gauges.

They do sampled observations, and we typically use them on request durations or response sizes,
as we've already seen on previous examples.

But they are more difficult to use **correctly**.

Soo, what's the problem with cardinality here?

Underneath, a single histogram includes a couple of counters with labels.
A counter per bucket, plus *sum* and *count* counters for all observed values.

By default histograms expose 10 buckets, so 12 counters in total.

Let's see an example.

</Notes>

---

<Notes>

Again let's start with adding our good old http request duration metric.
Let's say we ran our loadbalancer for a while.

1. Let's check our metrics. *It works*.
	`But there's something slightly off here`, on average requests take longer than we anticipated.

	We don't have enough visibility for the requests that take longer.

1. So let's add some more metrics, to get more granular picture.
	It's fine just a couple of more time series per histogram.

1. Let's check our metrics again.
	It looks better, however we still don't have enough visibility. Our requests still take longer.

1. We still don't have enough granularity, let's add more to cover the range of our latencies.

1. Now, it looks better. Let our system run for awhile.

</Notes>

<CodeSurfer>

```go title="Pitfall #6: Histogram Cardinality Explosion"

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="Let's check our metrics"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 8
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 302
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 3230.08239999999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 302

```

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="We need more buckets" 3,17

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #6: Histogram Cardinality Explosion"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 8
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="6"} 30
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="9"} 33
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="20"} 38
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="30"} 39
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="60"} 41
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="90"} 45
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="120"} 47
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 302
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 3230.08239999999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 302

```

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="More, and more ..." 3

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 40, 50, 60, 90, 120, 150, 200}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #6: Histogram Cardinality Explosion"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 8
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="6"} 30
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="9"} 33
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="20"} 38
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="30"} 39
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="60"} 41
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="90"} 45
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="120"} 47
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="150"} 118
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="200"} 258
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 302
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 3230.08239999999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 302

```

</CodeSurfer>

---

import itisfine from './static/itisfine.jpg'

<img src={itisfine} style="height: 75%; margin-top: 3%"/>

<Notes>

**It's fine, we got this covered!**

</Notes>

---

<CodeSurfer>

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="Anything you add, builds up."

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.01"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="9"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="20"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="30"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="40"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="50"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="60"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="90"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="120"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="150"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="200"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="+Inf"} 2
http_request_duration_seconds_sum{code="200",handler="/lb",method="post"} 0.0026940999999999996
http_request_duration_seconds_count{code="200",handler="/lb",method="post"} 2
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="9"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="20"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="30"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="40"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="50"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="60"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="90"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="120"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="150"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="200"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 146
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.3082099000000001
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 146
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-500-sometimes",method="post"} 4.01e-05
http_request_duration_seconds_count{code="200",handler="demo-500-sometimes",method="post"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-refused-conn-sometimes",method="post"} 4.46e-05
http_request_duration_seconds_count{code="200",handler="demo-refused-conn-sometimes",method="post"} 1


```

</CodeSurfer>

<Notes>

We started to see increased memory consumption in our system and we checked our metrics to see what's happening.
With histograms, cardinality multiplies very quickly!

Be very considered with your histograms.

Choose what you put in your labels wisely, especially when you are using histograms!

**And do not just choose your histogram buckets randomly.**

Which is actually gets me to my next point.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #7: Poorly Chosen Histogram Buckets</h2>

<Notes>

Histograms are great to collect observations from your system.

They can be used to produce arbitrary quantile estimations and to estimate the mean.

Because they accumulate their observation into buckets,
they have significantly **fewer storage requirements** compare to storing raw data.

As a result performance and the **accuracy** of your histograms tightly depend on your bucket layout.

Let's see some examples.

</Notes>

---

<Notes>

Again let's start with adding our good old http request duration metric.

1. Let's create an arbitrary bucket layout that we think could work.

1. Let's check our metrics. So what's wrong here.

	Prometheus implements histograms as **cumulative histograms**.
	*This means each bucket is a counter of observations less than or equal to your specified boundary*.
	Each bucket contains the counts of all prior buckets.

	As you see in this example, we are observing wrong interval for request durations.

1. Let's fix it.

	By using one of the convenience methods from Prometheus client library.

1. Looks better.

	So coming up with a good bucket layout is hard. You need to **know your distribution**.

	**Accuracy** is controlled by the granularity of your bucket layout.

	On the other hand, adding more bucket *increases cardinality by magnitudes*.

	Which leads performance problems.

My suggestion define your service level objectives and create your buckets accordingly.

For example, set the `highest bucket` to whatever **value that violates your SLAs**

and don't care about requests that take longer than that.

</Notes>

<CodeSurfer>

```go title="Pitfall #7: Poorly chosen Histogram buckets"

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Let's define our initial bucket layout" 3

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="So what's wrong here?"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 0.11527970000000001
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 49

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Let's fix it." 3:5

var (
	// prometheus client libraries are your friends!
	buckets = prometheus.LinearBuckets(0.001, 0.0002, 6)
	// buckets = prometheus.LinearBuckets(0.001, 2, 6)
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Know your distribution."

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0012000000000000001"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0014000000000000002"} 10
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0016000000000000003"} 21
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0018000000000000004"} 25
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0020000000000000005"} 29
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 0.10939249999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 49

```

</CodeSurfer>

---

### In Summary

<Notes>

1. Observe your applications, *monitoring is not optional*.

	You have to know what's going on with your application on production.

	Determine your service level objectives.

	Build alerts on them.

	Build dashboards on them.

2. Now, since you have also alerts and dashboards in place.

	You rely on them. They are liability.

	Test them as you test your business logic.

3. Last but not least,

	**Avoid global state**, make your life easier, *for yourself*.

	"magic is bad; global state is magic" by Peter Bourgon

</Notes>

<Appear>
	<h2>Monitoring is essential.</h2>
	<h2>Unit Test Your Instrumentation.</h2>
	<h2>Avoid Global Registry.</h2>
</Appear>

---

import ss_repo from './static/ss_repo.png'

##### [https://github.com/observatorium/observable-demo](https://github.com/observatorium/observable-demo)

<Image src={ss_repo} size='contain' />

<Notes>

If you want to learn more, try it yourself or dig deeper in the code,

here is a link to our fully-functional loadbalancer service.

</Notes>

---

# Thank you!

import red_hat_white from './static/red_hat_white.png'

<img src={red_hat_white} style="height: 20%"/>

<div style="font-size: 70%">

##### [https://github.com/kakkoyun/are-you-testing-your-observability](https://github.com/kakkoyun/are-you-testing-your-observability)

</div>

<Notes>

I think that's it.

Thank you very much for listening.

Feel free to ask questions.

</Notes>

---

# References:

<div style="font-size: 90%">

* [Prometheus](https://prometheus.io)
* [Prometheus - client_go](https://godoc.org/github.com/prometheus/client_golang/prometheus)
* [Thanos](https://thanos.io)
* [Why globals are magic](https://peter.bourgon.org/blog/2017/06/09/theory-of-modern-go.html)
* [Red Method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services)
* [Prometheus - Histogram](https://prometheus.io/docs/practices/histograms/)
* [Prometheus Histograms – Past, Present, and Future](https://www.youtube.com/watch?v=7sQFkaMCyEI)
* [Roboust Perception Blog](https://www.robustperception.io/blog)

</div>

