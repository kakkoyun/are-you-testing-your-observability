import './styles.css'

import { Head, Image, Appear, Notes, Invert, Split } from "mdx-deck"

import { CodeSurfer, CodeSurferColumns, Step } from "code-surfer"
import { shadesOfPurple } from "@code-surfer/themes"
import theme from './theme'

import thanos from './static/thanos_logo.svg'
import prometheus from './static/prometheus_logo.svg'
import redhat from './static/red_hat_logo.png'

import ss_tsdb_01 from './static/ss_tsdb_01.png'

export const themes = [
	shadesOfPurple,
	theme,
];

<Head>
	<title>Are you testing your Observability?</title>
</Head>

<CodeSurfer>

```go 6[17:53],7[17:53],11[17:53]
package main

import "fmt"

func main() {
	fmt.Println("Are you testing your Observability?")
	fmt.Println("      --- Metrics Edition ---      ")



	fmt.Println("    GoDays 22.01.2020, Berlin      ")
}
```

</CodeSurfer>

<Notes>

Hello everyone!

We are extremely excited to be here in GoDays conference, and be able to speak about topics we both love:

* observability
* programming in go

At the end of this talk we would like you to know:

* why instrumenting Go applications with actionable metrics is essential
* how to do it quickly and smoothly in Go, how to test it
* last but not the least: what are the common mistakes you should avoid, mistakes that
we seen a lot during our work with Go and metrics in WILD open source WORLD.

But before that: Short introduction!

</Notes>

---

<div style="width: 100%; height: 50%; overflow: auto;">
<img src="https://storage.googleapis.com/gopherizeme.appspot.com/gophers/1a34872cf0ec375b9fc44ce654fc03a5abc42dc4.png" style="height: 90%; float: left;"/>

#### Bartek Plotka

<div style="font-size: 80%">
Principal Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Prometheus and Thanos Maintainer<br/><br/>

<img src="https://raw.githubusercontent.com/kakkoyun/are-you-testing-your-observability/master/static/twitter.png" style="height: 40px; width: 40px;"/> <img src="https://raw.githubusercontent.com/kakkoyun/are-you-testing-your-observability/master/static/github.png" style="height: 40px; width: 40px;"/> @bwplotka
</div>
</div>

<div style="width: 100%; height: 50%; overflow: auto;">
<img src="https://storage.googleapis.com/gopherizeme.appspot.com/gophers/9806438d5acfb3a108eeaab302de0e32f8a489ad.png" style="height: 90%; float: left;"/>

#### Kemal Akkoyun

<div style="font-size: 80%">
Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Thanos Contributor<br/><br/>

<img src="https://raw.githubusercontent.com/kakkoyun/are-you-testing-your-observability/master/static/twitter.png" style="height:40px; width: 40px;"/> @kkakkoyun <span/>
<img src="https://raw.githubusercontent.com/kakkoyun/are-you-testing-your-observability/master/static/github.png" style="height: 40px; width: 40px;"/> @kakkoyun
</div>
</div>

<Notes>

My name is Bartek Plotka, I am an engineer working at Red Hat in the Monitoring team, I love open source and Go.
I am in Prometheus team and I am a co-author of Thanos project, which is a durable system for scaling Prometheus.

With me there is Kemal.

Hello everyone, my name is Kemal Akkoyun. I am also a software engineer working with Bartek at Red Hat, in the OpenShift Monitoring team.
I'm also into everything related to Go, Prometheus and Kubernetes. I love working with distributed systems and observability tools.
I try to contribute to all of them, as much as I can.

</Notes>

---

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={prometheus} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
</div>

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={thanos} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
</div>

<Notes>

We are building scalable Observability solutions and platforms for OpenShift.
But also as one of the major part of our work is maintaining Prometheus and Thanos projects on a daily basis.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's implement an HTTP L7 loadbalancer in Go! ‚ù§ </h2>

Ô∏è*Because why not?*

<Appear>
   <img src="https://docs.google.com/drawings/d/e/2PACX-1vQKHs_qMJWPKulEUxoDcLXww4Kq32IcJfPLbnnBfUDMj3AxQzuhRJtCKbw-i6TgqhBoRCfWo7RnV1nm/pub?w=960&h=720"/>
</Appear>

<Notes>

But today will be fun! We will talk today about building loadbalancer today. Kind of.

For demo purposes let's imagine we want to implement application level HTTP loadbalancer in Go?

Why?
Well these days with replicated microservices loadbalancer are crucial. HTTP is the most popular
protocol this days for API communication, and application level loadbalancing gives us more control
in terms of things like auditing, request based load balancing instead of connection based, graceful shutdowns,
inspection and distribution of load.

Appear:

So let's say we implemented transparent loadbalancer as presented in this diagram. From high level design we have:

> Single HTTP server that implements ServerHTTP method, so handler via awesome ReverseProxy in standard httputil package.
> ReverseProxy allows us to implement custom Transport, so RoundTripper interface which is to send
some request to remote backend.
> Our load balancing RoundTipper implementation called lbtranport is internally using then few components:
>> Discoverer which gives us targets to proxy / loadbalance request to
>> RoundRobinPicker which chooses right target to proxy user request to in round robin manner, so: replica 1, 2,3, then again 1, 2, 3
>> And at the end it uses HTTP transport so, http.RoundTripper that belongs to picked target and proxy request through and proxy response back to the user.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vQKHs_qMJWPKulEUxoDcLXww4Kq32IcJfPLbnnBfUDMj3AxQzuhRJtCKbw-i6TgqhBoRCfWo7RnV1nm/pub?w=960&h=720"/>

<Notes>

This is great, it looks like this implementation should work.. but are we sure it's production ready?

So let's say we deploy couple of replicas of our loadbalancer on production in front of some microservice and let
it run for longer time.

As soon as it starts running, we hit /lb endpoint manually and we can it works. So we are good, right?

Well...

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vT4N2dSOi0FhqFx1yVeNYPlJk6kT8o-7FdgvXvwpXiWVkK9MKM4SLflq4o3rJni9hEjWjrs7HKg0iOr/pub?w=960&h=720"/>

<Notes>

Maybe not really. It works for me but are we sure loadbalancer works as expected for all the time?
Does it actually work for all the users or only me? How many Bad Gateway Errors it was returning over time?
We can't really tell!

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vTOKGrz6rfXVoHSfdNY4Pe901pdaJizjylwqyhkwMU53bRy9VXAJE4hTfGJrGiDncfg3Coh7Par67lu/pub?w=960&h=720"/>

<Notes>
And what about discovery logic. Is DNS working? How many replicas it discovers?
How many it was discovering yesterday at 5pm?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vR-mA1zdYoQ-6pPjcSZFGTNIH1RV_3_VymMYOW1Y_OblYxiNoW-R-PBIVGHjNTHeyqiA-H9GIcZnQ6p/pub?w=960&h=720"/>

<Notes>

Is round robin picker, picking in round robin matter?
Is 1/3 of all requests actually going to replica 1? What's was the distribution overtime?
Is one backend is overloaded with request despite loadbalancing?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vTxiQx99iCG7WgA2sfceWofNnol0ynukyDZ3iCoOY02CDcyvkHIJC4J4SQvSYMYAbiDjbhs_3WWsGCr/pub?w=960&h=720"/>

<Notes>

What if users reports that the endpoint is slow: is it the backend that is slow?
Or is loadbalancing logic that is introducing the latency?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vRz638FUDhV2SGsC5bco5wVnE0VHb_OLU6g4SaOL108D1KZLXkzmTgSHz6d-d-OPYkij2BzA27FVio2/pub?w=960&h=720"/>

<Notes>

What about resource consumption of the loadbalancer?
What if one of the nodes that loadbalancer running on OOM-ed. Was loadbalancer responsible?
Maybe we have a memory leak?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vTTmGW2ky_qxu9TEYUsnvQwHBe0lhTSc07UHHSCRsjHldVwm2fR2GpIEHZkZD5HbW3hObU4TXJnN7N4/pub?w=960&h=720"/>

<Notes>
Finally what version of the loadbalancer we were running 2 days ago at 2pm? Maybe something was wrong back then and we
are not sure what version was actually rolled on Kubernetes...

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> We need: Observability!</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vSvJ_htd2Q-qEzCkxjl057HEiGpnP97JLSdHVtjqlH4_huRIp8kgmhD0vRbufTCF4UWjkuje-l2Lli8/pub?w=960&h=720"/>

<Notes>
As you can see there are massive amount of questions that would be not answered when running the service like this on
production, without proper monitoring.

That's why in SRE book you will find monitoring as the foundation of any system, BEFORE even the system itself!

As you might be familiar, some monitoring signals we can introduce are: traces, logs and metrics.
Guess which signal will give us answer to our questions like distribution of requests or histogram of tail latency?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>
<br/>
<br/>

<ul>
<Appear>
<li>Cheap</li>
<li>Near Real Time</li>
<li>Actionable (Alert-able)</li>
<div>
    <img src={prometheus} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
    <p>http://prometheus.io</p>

</div>
</Appear>
</ul>

<Notes>

Metrics, yup!

Metrics most likely give us the answers to our questions.
Answer that is in comparison to logs and traces:

1) CHEAPer to calculate
Near Real Time
Clear and Actionable, so you can alert on those.

In practice metrics should be the first item on monitoring list that you should do if you care to run your service reliably.

2) Why Prometheus though? Well I might be bias but Prometheus is currently one of the simplest and
cheapest option for collecting, storing and querying metrics as well as reliable alerting.
It is part of CNCF, fits for small solution as well as for bigger ones with help of cloud native projects like Thanos, Cortex, m3db and others

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vT4N2dSOi0FhqFx1yVeNYPlJk6kT8o-7FdgvXvwpXiWVkK9MKM4SLflq4o3rJni9hEjWjrs7HKg0iOr/pub?w=960&h=720"/>

<Notes>

So.. how to add Prometheus metrics to our loadbalancer?
Let's say we want to answer for how many users our loadbalancer is actually working?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<img src="https://docs.google.com/drawings/d/e/2PACX-1vREgXi-KbyUIdbSbaFsIQbkWE7ZrcPA0cBya53PB-jfIRL1E6uMvJC4HUc4Ca9Rujizv_-zQhEr3P_O/pub?w=960&h=720"/>

<Notes>

We can do that by incrementing some certain http_requests_total counter whenever a loadbalancing request occur, reporting method that was used, and HTTP status code that was returned.

We can do that in few simple steps.

</Notes>

---

<CodeSurfer>

```go title="Server HTTP request counter"

import "github.com/prometheus/client_golang/prometheus"

```

```go 5,6,7,8,9,10 title="Server HTTP request counter"

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

```

```go 13,14,18,19 title="Server HTTP request counter"

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServerHTTP handler.
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
  statusRec := newStatusRecorder(w)
  next.ServeHTTP(statusRec, r)

  // Increment our counter with written status code and request method.
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

```go 23 title="Server HTTP request counter"

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServerHTTP handler.
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
  statusRec := newStatusRecorder(w)
  next.ServeHTTP(statusRec, r)

  // Increment our counter with written status code and request method.
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

```

</CodeSurfer>

<Notes>

1) First of all we have to import Prometheus Go client library.
During our talk we will be talking mainly about Prometheus client Go module, which is designed to help
with instrumenting any Go application. It's tiny, heavily optimized and quite simple.

2) As a next step we need to define variable for the our counter of requests. We pick a name, a description
and certain "labels" which will be our dimensions for this counter. Each unique value in any of those labels will
result in totally new series in Prometheus system.

3) Next step is to actually count our requests. Let's create a simple wrapper of http server which will increment
the counter with the status code that the server returned and requested method.

4) Something that is easy to forget is another step: The counter has to be registered somewhere in order
to be exposed for Prometheus. Let's do this once in `init` function and register it in GLOBAL
Prometheus registry.

</Notes>

---

<CodeSurferColumns>

<Step title="Server HTTP request counter" subtitle="Why we need to register?">

```go 3

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

```

<img src="https://docs.google.com/drawings/d/e/2PACX-1vREgXi-KbyUIdbSbaFsIQbkWE7ZrcPA0cBya53PB-jfIRL1E6uMvJC4HUc4Ca9Rujizv_-zQhEr3P_O/pub?w=960&h=720"/>

</Step>
<Step title="Server HTTP request counter" subtitle="Registry is used by Prometheus /metrics handler">

```go 7,12

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

mux := http.NewServeMux()
mux.Handle("/metrics", promhttp.Handler())
mux.Handle("/lb", ...)

// Other handlers...

srv := &http.Server{Handler: mux}

// Run server...

```

<img src="https://docs.google.com/drawings/d/e/2PACX-1vRcDyIrN_3tzI_QbA99QkvMHAHDIZZi-sh1stXoul_M-zyM1GQPdOCvC_h8nU91l-uQ-UUKjhuGhlYj/pub?w=960&h=720"/>

</Step>
</CodeSurferColumns>

<Notes>

1) Let's focus on what we accomplish by registering this metric. It's important for the next step which is

2) HTTP handler for metric page. It serves simple text page with all metrics.
Once we add that to our loadbalancer, our server  is correctly exposing the http_requests_total counter
we created to the outside world.

</Notes>

---

import graph_requests from './static/graph-requests.png'

<CodeSurfer>

```go 4,6 title="From code to graph" subtitle="We defined & instrumented our metric"
var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

```

```go 4,6,10,13,14 title="From code to graph" subtitle="It is now exposed under /metrics"
var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Part of response from /metrics HTTP endpoint.
# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="get"} 1089
http_requests_total{code="500", method="get"} 46
```

</CodeSurfer>
<Notes>

1) How we can now use our metric? As you remember we defined this metric like this and we increment it every HTTP request.
2) Loadbalancer now serves /metrics page which exposes our metrics in Prometheus supported text format.
As you can see each code and method are a separate counter, with mostly successes.

</Notes>

---

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Prometheus can now collect metrics from our Loadbalancer</h2>

*Pulling e.g every 15 seconds*

<img src="https://docs.google.com/drawings/d/e/2PACX-1vQazezdwEI7-_Naf_aMSGipdrSNMoDgc9YtrFyO_ttl1kuyeI7jn8lFtfVk8jQ35BQEAg2m8CvYGp3r/pub?w=590&h=701"/>

<Notes>

We now can use running binary of Prometheus and point to the loadbalancer /metrics page. Prometheus then will
visit this page (which is called scrape) every given interval and collect all exposed metrics.

</Notes>

---

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Graph: Prometheus UI</h2>

<img src={graph_requests} style="height: 70%; margin-top: 5%"/>

<Notes>

With that we can after some time, visit Prometheus UI where we can produce graphs. For example we can query
the number of requests per minute by code and method. We can see that per minute we have 120 requests in total,
with some small portion of those being error responses.

</Notes>

---

import pitfalls from './static/pitfall.gif'
import golang from './static/go.png'

<div>
    <h1 style="text-align: center; margin: 0px 10px 0 10px;">
        <img src={golang} style="width: 7%; margin-top: 5%"/>
        &nbsp;+&nbsp;
        <img src={prometheus} style="width: 10%; margin-top: 5%"/>
        &nbsp;&nbsp;Pitfalls
    </h1>
</div>

<img src={pitfalls} style="height: 50%; margin-top: 5%"/>

<Notes>
So it looked easy right? And it is easy in most cases. However during this talk we would like
to present what we learnt during few year, while developing and reviewing instrumentation Go code that
is meant to be run on production, mainly in open source.

Together with Kemal wil go though few less or more advanced issues we seen and how to resolve them.
</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #1: Global Registry</h2>
<br/>
<br/>
<h6 style="text-align: center; margin: 0 10px 0 10px;">"magic is bad; global state is magic" by Peter Bourgon</h6>

<Notes>
First one! Globals.

There was a saying in Peter Bourgon blog post "A theory of Modern Go": magic is bad; global state is magic.

This is very true in case of Prometheus client, especially if you are instrumenting some Go package with metrics
that your project, or maybe anyone in open source is using.

So let's focus on what can go wrong when using Globals when instrumenting our loadbalancer with metrics.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #1: Global Registry" subtitle="We have 2 global variables here."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

  // ...
  // Increment our counter with written status code and request method.
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```diff 3,4,5,6,7,8 title="Pitfall #1: Global Registry" subtitle="Package-level metric variable."
```

```go 12[16:80] title="Pitfall #1: Global Registry" subtitle="and global DefaultRegisterer."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

  // ...
  // Increment our counter with written status code and request method.
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go 15,16,17,18,19,20,21,22 title="Pitfall #1: Globals: No flexibility" subtitle="What if I have more handlers than one?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

  // For endpoint /one:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Globals: No flexibility" subtitle="Nice, but for what handler?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

  // For endpoint /one:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  # HELP http_requests_total Tracks the number of HTTP requests.
  # TYPE http_requests_total counter
  http_requests_total{code="200", method="get"} 2445
  http_requests_total{code="500", method="get"} 53

```

```go title="Pitfall #1: Getting rid of globals."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

  // For endpoint /one:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals." subtitle="Introduce instance of metrics!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

  // For endpoint /one:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals." subtitle="Create new instance of it"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

  metrics := NewServerMetrics()

func init() {
	prometheus.DefaultRegisterer.MustRegister(metrics.requestsTotal)
}

  // For endpoint /one:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals." subtitle="Register using Custom Registerer (composition!)"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

  reg := prometheus.NewRegistry()
  metrics := NewServerMetrics(reg)

  // For endpoint /one:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

``` go title="Pitfall #1: Getting rid of globals." subtitle="Now it's package-level user friendly!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

  reg := prometheus.NewRegistry()
  metrics := NewServerMetrics(reg)

  metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
  )
  metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
  )
  metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
  )

  // For endpoint /one:
  metrics1.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  metrics2.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  metrics3.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals." subtitle="We can have request counter per handler (:"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

  reg := prometheus.NewRegistry()
  metrics := NewServerMetrics(reg)

  metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
  )
  metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
  )
  metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
  )

  // For endpoint /one:
  metrics1.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /two:
  metrics2.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  // For endpoint /three:
  metrics3.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

  # HELP http_requests_total Tracks the number of HTTP requests.
  # TYPE http_requests_total counter
  http_requests_total{handler="/one", code="200", method="get"} 1445
  http_requests_total{handler="/one", code="500", method="get"} 23
  http_requests_total{handler="/two", code="200", method="get"} 445
  http_requests_total{handler="/two", code="500", method="get"} 0
  http_requests_total{handler="/three", code="200", method="get"} 645
  http_requests_total{handler="/three", code="500", method="get"} 40

```

</CodeSurfer>

<Notes>

1) Let's take our example or http requests_total metric. We have 2 global states here.

2) As you can see metric is a global, package level variable. We register it once per package import as well.

3) Second place of global state is MustRegister which is actually hiding a Global DefaultRegisterer.

Now what's the issue here? Why is that problematic?

4) No flexibility! What if you have more than one hanler, more than one endpoint?

5) As you can see the inight you gain is pretty limited as the requests are counted per all endpoints. I can't tell
what's the error rate for /three endpoint for example.

Let's try to fix this!

6) And by fixing I mean removing globals!

7) Let's replace global variabl with some struct that you can instantiate. It will have constructor that will create
our counter.

8) Now you can create such object and use it everywhere.

9) To eliminate last ..

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #2: No Testing</h2>

---

<CodeSurfer>

```go title="Not testing"

type Metrics struct {
	successes prometheus.Counter
	failures  *prometheus.CounterVec
	duration  prometheus.Histogram
}

func NewMetrics(reg prometheus.Registerer) *Metrics {
	m := &Metrics{
		successes: prometheus.NewCounter(prometheus.CounterOpts{
			Subsystem: "lbtransport",
			Name:      "proxied_requests_total",
			Help:      "Total number successful proxy round trips.",
		}),
		failures: prometheus.NewCounterVec(prometheus.CounterOpts{
			Subsystem: "lbtransport",
			Name:      "proxied_failed_requests_total",
			Help:      "Total number failed proxy round trips.",
		}, []string{"reason"}),
		duration: prometheus.NewHistogram(
			prometheus.HistogramOpts{
				Subsystem: "lbtransport",
				Name:      "proxy_duration_seconds",
				Help:      "Duration of proxy logic.",
				Buckets:   []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 10},
			}),
	}

	if reg != nil {
		reg.MustRegister(m.successes, m.failures, m.duration)
	}

	return m
}

```

```go title="prometheus/testutil here to help"

import (
	"github.com/prometheus/client_golang/prometheus/testutil"
)

```

</CodeSurfer>

<Notes>
</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #3: Lack of Consistency</h2>
<br/>
<br/>
<Appear>

<h5 style="text-align: center; margin: 0 10px 0 10px;">The Four Golden Signals, USE method, RED method etc...</h5>

<div>
<br/>
<br/>
<ul>
<li><span style="color: red">R</span>: Requests per second (saturation).</li>
<li><span style="color: red">E</span>: Errors per second.</li>
<li><span style="color: red">D</span>: Duration (tail latency).</li>
</ul>
</div>
</Appear>

<Notes>

Lack of consistency. There where certain methods defined, like ...

1) This helps to be sure you have main signals upfront of potential debugging or alerting needs.
2) It helps to reuse common "RED method type" alerts, recording rules and dashboards. e.g mixin project.

Let's focus on one method. RED.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #3: Lack of consistency" subtitle="Does this satisfy RED method?"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```diff 11,13 title="Pitfall #3: Lack of consistency" subtitle="R = Requests"
```

```diff 13[25:30] title="Pitfall #3: Lack of consistency" subtitle="E = Errors (code =! 2..)"
```

```diff 10[1:2] title="Pitfall #3: Lack of consistency" subtitle="D = Duration is Missing!"
```

```go 4,16,17,18,19,20,21,22,23,24,32,33 title="Pitfall #3: Consistency" subtitle="Red method satisfied"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
        requestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "http_request_duration_seconds",
                Help:    "Tracks the latencies for HTTP requests.",
                Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120},
            },
            []string{"code", "method"},
        ),
	}
	reg.MustRegister(m.requestsTotal, m.requestDuration)
	return ins
}

// Top level ServerHTTP handler.
func ServerHTTP(w http.ResponseWriter, r *http.Request) {
  start := time.Now()
  defer metrics.requestDuration.WithLabelValues(statusRec.Status(), r.Method)).Observe(time.Since(start))

  statusRec := newStatusRecorder(w)
  next.ServeHTTP(statusRec, r)

  // Increment our counter with written status code and request method.
  metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

</CodeSurfer>

<Notes>

Does our code satisfy RED method?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #4: Not conforming naming convention</h2>

<br/>
<br/>
<ul>
<div style="font-size: 80%;">
<Appear>
<li><a href="https://prometheus.io/docs/practices/naming/#metric-and-label-naming">https://prometheus.io/docs/practices/naming/#metric-and-label-naming</a></li>
<li>
Name should have a suffix describing the unit, in plural form. Note that an accumulating count has *_total* as a suffix, in addition to the unit if applicable
Examples: <br/>
<div style="font-style: italic;">
<ul>
<li>http_request_duration_<b>seconds</b></li>
<li>node_memory_usage_<b>bytes</b></li>
<li>http_requests_<b>total</b></li>
<li>process_cpu_<b>seconds_total</b></li>
<li>build_<b>info</b></li>
</ul>
</div></li>
</Appear>
</div>
</ul>
<Notes>

There is official Prometheus metric and label name convention!

1. Something to rememeber

</Notes>

---

<CodeSurfer>

```go title="Pitfall #4: Naming: stability"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}
```

```go title="Pitfall #4: Naming: stability" subtitle="Let's say we alert on too many 5xx responses."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go 11[29:37] title="Pitfall #4: Naming: stability" subtitle="Let's say we are renaming metric..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go title="Pitfall #4: Naming: stability" subtitle="Ups..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

## BOOM! This alert will never fire but also will not fail!
## Rename can cause issues like this in Alerts, Recording rules, Dashboards and more..
alert: HttpToMany502Errors
  expr: |
    sum(rate(http_requests_total{status="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with status 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

</CodeSurfer>

<Notes>

There is important aspect of naming as well: its stability.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #5: Unbounded Cardinality</h2>

<Notes>

KEMAL

When we talk about Prometheus it always comes down to cardinality.

What is cardinality actually? So the cardinality, in Prometheus context, is the amount of unique time-series you have in youir system.

And don't forget each label value that you add to your metric creates another time=series.

So for example, if you have a label containing HTTP methods would have a cardinality of 2 if you had only GET and POST in your application.

Labels what make Prometheus stong. However you should always watch out how you use them.
Things could get out of control pretty quickly.

Let's see an example.

</Notes>

---

<CodeSurfer>

```go title="Unbounded Cardinality" subtitle="Define a metric" 7:14

type DialerMetrics struct {
	connFailedTotal *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

```

```go 24:28 subtitle="Open a connection"

type DialerMetrics struct {
	connFailedTotal *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(err).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

```

```git subtitle="Let's check out our metrics... üò≤"
# HELP conntrack_dialer_conn_failed_total Total number of connections failed to dial by the dialer.
# TYPE conntrack_dialer_conn_failed_total counter
conntrack_dialer_conn_failed_total{reason="<nil>"} 1
conntrack_dialer_conn_failed_total{reason="lookup example.com on localhost: err..."} 1
conntrack_dialer_conn_failed_total{reason="lookup thanos.io on 8.8.8.8: err..."} 1
conntrack_dialer_conn_failed_total{reason="lookup redhat.com on localhost: err..."} 1
conntrack_dialer_conn_failed_total{reason="syscall: unimplemented EpollWait"} 1
conntrack_dialer_conn_failed_total{reason="syscall.SIGINT: series of unfortunate things happened"} 1
conntrack_dialer_conn_failed_total{reason="unix: test returned fd in TestEpoll"} 1
conntrack_dialer_conn_failed_total{reason="Invalid value for argument: client: nil"} 1
```

</CodeSurfer>

<Notes>

KEMAL

Let's define a metric called `conntrackdialer_conn_failed_total`, to track the number of failed connections we have in our loadbalancer.

And let's have a label called `reason` to track failure reasons.

And then open our connection and whatever needs to be done and if we fail, let's increment our metric with corresponding error.

Everything looks good, let's run our loadbalancer and get our metrics.

</Notes>

---

<CodeSurfer>

```go 5:11,22:25 subtitle="How can we improve this?"

// ...

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(err).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

```

```go subtitle="Never use arbitrary data as label values"

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

// ...

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(err).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

```

```go

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

// ...

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(dialErrToReason(err)).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

```

```go subtitle="Never use arbitrary data as label values"

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

// ...

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(dialErrToReason(err)).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

func dialErrToReason(err error) string {
	if netErr, ok := err.(*net.OpError); ok {

		switch nestErr := netErr.Err.(type) {
		case *net.DNSError:
			return failedResolution

		case *os.SyscallError:
			if nestErr.Err == syscall.ECONNREFUSED {
				return failedConnRefused
			}
			return failedUnknown
		}

		if netErr.Timeout() {
			return failedTimeout
		}
	}

	return failedUnknown
}

```

```go 42[14:27],43,46[15:50],47,49,52[9:27],53,57

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

// ...

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

func dialClientConnTracker(...) (net.Conn, error) {
	conn, err := parentDialContextFunc(ctx, ntk, addr)
	if err != nil {
		metrics.connFailedTotal.WithLabelValues(dialErrToReason(err)).Inc()
		return conn, err
	}

	return &clientConnTracker{...}, nil
}

func dialErrToReason(err error) string {
	if netErr, ok := err.(*net.OpError); ok {

		switch nestErr := netErr.Err.(type) {
		case *net.DNSError:
			return failedResolution

		case *os.SyscallError:
			if nestErr.Err == syscall.ECONNREFUSED {
				return failedConnRefused
			}
			return failedUnknown
		}

		if netErr.Timeout() {
			return failedTimeout
		}
	}

	return failedUnknown
}

```

</CodeSurfer>

<Notes>

KEMAL

That is not what we expacted.
What went wrong?

We shoudn't use arbitrary data as label values.

We should always keep them in bounds.

A better approach would be to predefine them.

So let's see how we can fix it.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #6: Histogram Cardinality Explosion</h2>

---

<CodeSurfer>

```go title="Histogram Cardinality Explosion"

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Histogram Cardinality Explosion"

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="9"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="20"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="30"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="60"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="90"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="120"} 30
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 30
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.08239999999999999
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 30

```

```go title="Histogram Cardinality Explosion" 13

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 40, 50, 60, 90, 120, 150, 200},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="9"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="20"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="30"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="40"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="50"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="60"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="90"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="120"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="150"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="200"} 18
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 18
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.0480237
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 18

```

</CodeSurfer>

---

# üí•

<Notes>
	TODO: Kemal

	Bucket layouts
</Notes>

---

<CodeSurfer>

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.01"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="9"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="20"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="30"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="40"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="50"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="60"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="90"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="120"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="150"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="200"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="+Inf"} 2
http_request_duration_seconds_sum{code="200",handler="/lb",method="post"} 0.0026940999999999996
http_request_duration_seconds_count{code="200",handler="/lb",method="post"} 2
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="9"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="20"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="30"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="40"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="50"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="60"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="90"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="120"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="150"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="200"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 146
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.3082099000000001
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 146
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-500-sometimes",method="post"} 4.01e-05
http_request_duration_seconds_count{code="200",handler="demo-500-sometimes",method="post"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-refused-conn-sometimes",method="post"} 4.46e-05
http_request_duration_seconds_count{code="200",handler="demo-refused-conn-sometimes",method="post"} 1

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #7: Poorly Chosen Histogram Buckets</h2>

---

<Notes>

KEMAL

TODO: Kemal

	Histograms and summaries are more complex metric types. Not only does a single histogram or summary create a multitude of time series, it is also more difficult to use these metric types correctly. This section helps you to pick and configure the appropriate metric type for your use case.
	Histograms and summaries both sample observations, typically request durations or response sizes. They track the number of observations and the sum of the observed values, allowing you to calculate the average of the observed values.

Show metrics with yolo many buckets
TODO: Problem: Cardinality (explain why)
TODO: Solution: SLO based!
TODO: Define histograms!
TODO: Why do we need buckets?

Histograms are more complex metric types compare to metrics and gauges.
However underneath they are just a couple of counters with labels.

They do sampled observations, typically on request durations or response sizes.
They track the number of observations and the sum of the observed values, allowing you to calculate the average of the observed values.

They accummulate theur observation in to buckets. You have to define those buckets when you create them.
As a result performance of your histogram tightly depends on your bucket layout and coming up with a correct layout is more art then science :)

Lets see some bad bucket layouts and how to fix them.

</Notes>

<CodeSurfer>

```go title="Poorly chosen Histogram buckets"

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Poorly chosen Histogram buckets" 13

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 49
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.11527970000000001
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 49

```

```go title="Poorly chosen Histogram buckets" 13

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: []float64{0.0000001, 0.000001, 0.00001, 0.00003, 0.00006, 0.001, 0.03},
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1e-07"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1e-06"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1e-05"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3e-05"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6e-05"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.03"} 23
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 23
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.0636401
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 23

```

```go title="Poorly chosen Histogram buckets" 13

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: prometheus.LinearBuckets(0.001, 0.0002, 6),
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```git

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 3
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.0012000000000000001"} 3
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.0014000000000000002"} 10
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.0016000000000000003"} 21
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.0018000000000000004"} 25
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.0020000000000000005"} 29
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.10939249999999999
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 49

```

```go title="Poorly chosen Histogram buckets"

DefaultBuckets = []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10}
prometheus.ExponentialBuckets(.1, 1.5, 5),
prometheus.LinearBuckets(.01, .01, 10)

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #8: Not Initialized Metrics</h2>

<Notes>

KEMAL

Another common pitfall is not to initialize your metrics with labels.

Let's have a look at an example.

</Notes>

---

<CodeSurfer>

```go title="Not initializing Metrics" subtitle="Define a metric"

type DialerMetrics struct {
	connFailedTotal      *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

```

```go subtitle=""
m.connFailedTotal.WithLabelValues("unknown").Add(30)
```

</CodeSurfer>

<Notes>

As always let's start with defining a metric.

1)
An for this example as well let's use the same metric that we got familiar with `conntrack_dialer_conn_failed_total`.

2)
Let's set a value with a specfic reason for this counter.

</Notes>

---

import uninitialized_metrics from './static/uninitialized_metrics.png'

<Image src={uninitialized_metrics} size='contain'/>

<Notes>

KEMAL

As you can see, although we have our increment metric.

We do not see the actual change with our `increase` function.

</Notes>

---

import uninitialized_metrics_marked from './static/uninitialized_metrics_marked.png'

<Image src={uninitialized_metrics_marked} size='contain'/>

<Notes>

KEMAL

This is not something we want.

</Notes>

---

<Notes>

KEMAL

Let's fix this.

1)
We can start with predefining our labels as constants.
This is not strictly necessary but it always helps me.

2)
And then let's use `WithLabelValues` method to initialize our counters.

3)
Let's run it again see what happens.

</Notes>

<CodeSurfer>

```go title="Not initializing Metrics"

type DialerMetrics struct {
	connFailedTotal      *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

```

```go title="Not initializing Metrics"

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

type DialerMetrics struct {
	connFailedTotal      *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	return m
}

```

```go title="Not initializing Metrics"

const (
	failedResolution  = "resolution"
	failedConnRefused = "refused"
	failedTimeout     = "timeout"
	failedUnknown     = "unknown"
)

type DialerMetrics struct {
	connFailedTotal      *prometheus.CounterVec
}

func NewDialerMetrics(reg prometheus.Registerer) *DialerMetrics {
	m := &DialerMetrics{
		connFailedTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Subsystem: "conntrack",
				Name:      "dialer_conn_failed_total",
				Help:      "Total number of connections failed to dial by the dialer.",
			}, []string{"reason"}),
	}

	if reg != nil {
		reg.MustRegister(m.connClosedTotal)
	}

	// Initialize metric with labels.
	// It is possible to call this method without using the returned Counter to only
	// create the new Counter but leave it at its starting value 0.
	m.connFailedTotal.WithLabelValues(failedResolution)
	m.connFailedTotal.WithLabelValues(failedConnRefused)
	m.connFailedTotal.WithLabelValues(failedTimeout)
	m.connFailedTotal.WithLabelValues(failedUnknown)

	return m
}

```

```go title="Not initializing Metrics"
m.connFailedTotal.WithLabelValues("unknown").Add(30)
```

</CodeSurfer>

---

import initialized_metrics from './static/initialized_metrics.png'

<Image src={initialized_metrics} size='contain'/>

<Notes>

KEMAL

Now we see actual increment value.

</Notes>

---

import initialized_metrics_marked from './static/initialized_metrics_marked.png'

<Image src={initialized_metrics_marked} size='contain'/>

<Notes>

KEMAL

This is exactly what we want.

Now we can be sure that this counter could be used in alerts.

TODO: More clarification.

</Notes>

---

# Summary

<ul>
	<Appear>
		<li>Monitoring is essential.</li>
		<li>Avoid Global Registry.</li>
		<li>Unit Test Your Instrumentation.</li>
	</Appear>
</ul>

<Notes>
    TODO: Advertise our repo as separate slide
	TODO: Kemal
</Notes>

---

# Thank you!

<img src="https://raw.githubusercontent.com/kakkoyun/are-you-testing-your-observability/master/static/red_hat_white.png" style="height: 20%"/>

# Links:

* [Observable Demo](https://github.com/observatorium/observable-demo)
* [Slides](https://github.com/kakkoyun/are-you-testing-your-observability)

<Notes>

You can find working example of our demo and the slides in those links.
And we will be sharing our slides.

</Notes>

---

# Reference:

* [Thanos](https://thanos.io)
* [Prometheus](https://prometheus.io)
* [Prometheus - client_go](https://godoc.org/github.com/prometheus/client_golang/prometheus/testutil)
* [Prometheus - Histogram](https://prometheus.io/docs/practices/histograms/)
* [Prometheus Histograms ‚Äì Past, Present, and Future](https://www.youtube.com/watch?v=7sQFkaMCyEI)
* [Roboust Perception - Cardinality is the key](https://www.robustperception.io/cardinality-is-key)
* [Gopherize me](https://gopherize.me/)
* [Why globals are magic](https://peter.bourgon.org/blog/2017/06/09/theory-of-modern-go.html)
* [Red Method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services)
