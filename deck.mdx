import './styles.css'

import { Head, Image, Appear, Notes, Invert, Split } from "mdx-deck"

import { CodeSurfer, CodeSurferColumns, Step } from "code-surfer"
import { shadesOfPurple } from "@code-surfer/themes"
import theme from './theme'

import thanos from './static/thanos_logo.svg'
import prometheus from './static/prometheus_logo.svg'
import redhat from './static/red_hat_logo.png'

export const themes = [
	shadesOfPurple,
	theme,
];

<Head>
	<title>Are you testing your Observability?</title>
</Head>

<CodeSurfer>

```go 4,5,6,7,8,9,10

    # HELP talks_total Total number of talks given.
    # TYPE talks_total counter
	talks_total{title="    Are you testing your Observability?    "} 2
	talks_total{subtitle="       --- Metrics Edition ---       "} 2



	talks_total{conf="FOSDEM", when="02.02.2020", where="Brussels"} 1

```

</CodeSurfer>

<Notes>

Hello everyone!

We are extremely excited to be here in FOSDEM conference, and be able to speak about the topic we love Observability.

We hope our talk will be very inspiring and actionable for you.
This is because at the end of this talk we would like you to know 3 THINGS:

* Why instrumenting backend applications with actionable metrics is essential
* How to instrument your service quickly for Prometheus metric system to use
* And last but not the least: What are the common mistakes you should avoid, mistakes that
we seen a lot during our work with metrics in the amazing (but sometimes WILD) OPEN SOURCE WORLD.

But before that: Short introduction!

</Notes>

---

import bartek from './static/bartek.jpeg'
import kemal from './static/kemal.jpeg'
import twitter from './static/twitter.png'
import github from './static/github.png'

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={bartek} style="height: 90%; float: left; margin-top: 20px; padding: 0 20px 0 20px"/>

#### Bartek Plotka

<div style="font-size: 50%">
Principal Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Prometheus and Thanos Maintainer<br/><br/>

<img src={twitter} style="height: 40px; width: 40px;"/> <img src={github} style="height: 40px; width: 40px;"/> @bwplotka
</div>
</div>

<div style="width: 100%; height: 50%; overflow: auto;">
<img src={kemal} style="height: 90%; float: left; margin-top: 20px; padding: 0 20px 0 20px"/>

#### Kemal Akkoyun

<div style="font-size: 50%">
Software Engineer @ Red Hat<br/>
OpenShift Monitoring Team<br/>
Thanos Contributor<br/><br/>

<img src={twitter} style="height:40px; width: 40px;"/> @kkakkoyun <span/>
<img src={github} style="height: 40px; width: 40px;"/> @kakkoyun
</div>
</div>

<Notes>

My name is Bartek Plotka, I am an engineer working at Red Hat in the Monitoring team, I love open source and solving problems
using Go.
I am part of Prometheus Team and I am a co-author of Thanos project, which is a durable system for scaling Prometheus.

With me there is Kemal...

Hello everyone, my name is Kemal. I am also a software engineer in the OpenShift Monitoring team, at Red Hat.
I love working on things related to Go, Prometheus, Kubernetes, and I contribute to Thanos.

</Notes>

---

import d01 from './static/d01.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Let's implement an HTTP L7 loadbalancer! ‚ù§</h2>

<Appear>
	<Image src={d01} size='contain'/>
</Appear>

<Notes>

So.. let's have a fun task! We will talk today about building loadbalancer. Kind of.

For demo purposes let's imagine we want to implement application level HTTP loadbalancer. Let's say, we will do it in Go,
but programming language does not matter here.

1) So let's say we implemented transparent loadbalancer as presented in this diagram.
From high level design we have couple of Go components.

- Single HTTP server that implements ServeHTTP method, so handler via awesome ReverseProxy in standard httputil package.
- that ReverseProxy allows us to inject custom Transport
- We inject there our load balancing RoundTipper implementation called lbtranport, which is internally
using then few components:
-- Discoverer which gives us targets to proxy / loadbalance request to
-- RoundRobinPicker which chooses right target to proxy user request to in FAIR, round robin manner, so: replica 1, 2,3, then again 1, 2, 3
-- And at the end it uses http.Transport to forward request to picked replica and proxy response back to the user.

</Notes>

---

import d02 from './static/d02.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d02} size='contain'/>

<Notes>

This is great, it looks like this implementation should work.. but are we sure it's production ready?

So let's say we deploy couple of replicas of our loadbalancer in production in front of some microservice and let
it run for longer time.

As soon as it starts running, we hit /lb endpoint manually and we can see it works. So we are good, right?

Well...

</Notes>

---

import d03 from './static/d03.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d03} size='contain'/>

<Notes>

Not necessarily. It works for me but are we sure loadbalancer works as expected all the time?
Does it actually work for all the users or only me? How many Bad Gateway Errors it was returning over time?
We can't really tell!

</Notes>

---

import d05 from './static/d05.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d05} size='contain'/>

<Notes>

And what about round robin picker logic? Is it, picking in round robin matter?
Is 1/3 of all requests actually going to replica 2? What's was the distribution of request over time? Was it fair?

</Notes>

---

import d06 from './static/d06.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d06} size='contain'/>

<Notes>

What if users reports that the endpoint is slow: is it the backend that is slow?
Or is loadbalancing logic that is introducing the latency?

</Notes>

---

import d08 from './static/d08.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Wait... Anything missing?</h2>

<Image src={d08} size='contain'/>

<Notes>

Finally what version of the loadbalancer we were running 2 days ago at 2pm? Maybe something was wrong back then and we
are not sure what version was actually rolled on Kubernetes...

</Notes>

---

import d09 from './static/d09.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> We need Monitoring!</h2>

<Image src={d09} size='contain'/>


<Notes>

As you can see there are massive amount of questions that would be not answered when running the service like this on
production, without proper monitoring.

That's why in Site Reliability Engineering Book book you will find monitoring as the foundation of any system, BEFORE even the system itself!

As you might be familiar, some monitoring signals we can introduce are: traces, logs and metrics.
Guess which signal will give us answer to our questions like distribution of requests or histogram of tail latency?

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Let's instrument our LB with Prometheus metrics!</h2>

<ul style="font-size: 80%; margin-top: 95px">
<Appear>
<li>Cheap</li>
<li>Near Real Time</li>
<li>Actionable (Alert-able)</li>
<div>
    <img src={prometheus} style="height: 50%; margin: auto; display: block; margin-top: 100px;" />
    <p>http://prometheus.io</p>
</div>
</Appear>
</ul>

<Notes>

Metrics, yup!

Metrics most likely give us the answers to our questions.
Answer that is in comparison to logs and traces:

1) CHEAPer to calculate
Near Real Time
Clear and Actionable, so you can alert on those.

In practice metrics probably should be the first item on monitoring list that you should do if you care to run your service reliably.

2) Why Prometheus though? Well I might be bias but Prometheus is currently one of the simplest and
cheapest option for collecting, storing and querying metrics as well as reliable alerting.
It is part of CNCF, fits for small solution as well as for bigger ones with help of cloud native projects like Cortex, M3db or Thanos and others.

</Notes>

---

import d11 from './static/d11.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<Image src={d03} size='contain'/>

<Notes>

So.. how to add Prometheus metrics to our loadbalancer?
Let's say we want to answer for how many users our loadbalancer is actually responding working, and for how many
it returns error!

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's instrument our LB with Prometheus metrics!</h2>

<Image src={d11} size='contain'/>

<Notes>

We can do that by incrementing some certain http_requests_total counter whenever a loadbalancing request occur
reporting method that was used, and HTTP status code that was returned.

We can introduce this metric really in few simple steps.

</Notes>

---

<CodeSurfer>

```go title="Server HTTP request counter" subtitle="Import Prometheus Go client package."

// You can find list of all available clients for 18 programming languages here:
// https://prometheus.io/docs/instrumenting/clientlibs/#client-libraries
import "github.com/prometheus/client_golang/prometheus"

```

```go 5,6,7,8,9,10 title="Server HTTP request counter" subtitle="Define counter."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

```

```go title="Server HTTP request counter" subtitle="Instrument ServeHTTP function."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

```go 23 title="Server HTTP request counter" subtitle="Register our metric."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

```

```go title="Server HTTP request counter" subtitle="Add /metrics handler."

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

mux := http.NewServeMux()
mux.Handle("/metrics", promhttp.Handler())
mux.Handle("/lb", ...)

// Other handlers...

srv := &http.Server{Handler: mux}

// Run server...

```

```go title="Server HTTP request counter" subtitle="Metric is exposed on /metrics"

import "github.com/prometheus/client_golang/prometheus"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

mux := http.NewServeMux()
mux.Handle("/metrics", promhttp.Handler())
mux.Handle("/lb", ...)

// Other handlers...

srv := &http.Server{Handler: mux}

// Run server...

// Part of response from /metrics HTTP endpoint.
# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="get"} 1089
http_requests_total{code="500", method="get"} 46

```

</CodeSurfer>

<Notes>

1) We will show here an example on how to quickly add metric in Go language. However it's equally simple in other languages as well.

So let's first import Go client library. Similar libraries exists in 18 other programming languages.

2) To add server HTTP metrics, As a next step we need to define variable for the our counter of requests. We pick a name, a description
and certain "labels" which will be our dimensions for this counter. Each unique value in any of those labels will
result in totally new series in Prometheus system.

3) Next step is to actually count our requests. Let's create a simple wrapper of http server which will increment
the counter with the status code that the server returned and requested method.

4) Something that is easy to forget is another step: The counter has to be registered somewhere in order
to be exposed for Prometheus. Let's do that with Prometheus MustRegister method.

5) Then we have to ad HTTP handler for metric page under e.g /metrics. It serves simple text page with all
registered metrics.

6) Once we add that to our loadbalancer, our server is correctly exposing the http_requests_total counter
we created. As you can see each code and method are a separate counter, with mostly successes.

And this is exactly the text format that Prometheus expects..

</Notes>

---

import graph_requests from './static/graph-requests.png'
import d10 from './static/d10.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Prometheus can now collect metrics from our Loadbalancer</h2>

*Pulling e.g every 15 seconds*

<Image src={d12} size='contain'/>

<Notes>

We now can use running binary of Prometheus and point to the loadbalancer /metrics page. Prometheus then will
visit this page (which is called scrape) every given interval and collect all exposed metrics.

</Notes>

---

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Graph: Prometheus UI</h2>

<img src={graph_requests} style="height: 70%; margin-top: 5%"/>

<Notes>

With that we can after some time, visit Prometheus UI where we can produce graphs. For example we can query
the number of requests per minute by code and method. We can see that per minute we have 120 requests in total,
with some small portion of those being error responses.

</Notes>

---

import pitfalls from './static/pitfall.gif'

<div>
    <h1 style="text-align: center; margin: 0px 10px 0 10px;">
        <img src={prometheus} style="width: 10%; margin-top: 5%"/>
        &nbsp;&nbsp;Pitfalls
    </h1>
</div>

<img src={pitfalls} style="height: 50%; margin-top: 5%"/>

<Notes>

So it looked easy right? And it is easy in most cases. However during this talk we would like
to present what we learnt during couple of years of developing and reviewing instrumentation code that
is meant to be run on production, in close but mainly in open source.

Together with Kemal wil go though a few less or more advanced issues we seen and how to resolve them.

</Notes>

---

import magic from './static/magic.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #1: Global Registry</h2>
<br/>
<br/>
<h6 style="text-align: center; margin: 0 10px 0 10px;">"magic is bad; global state is magic" by Peter Bourgon</h6>
<br/>
<img src={magic} style="height: 50%; margin-top: 5%"/>

<Notes>

First one! Globals.

There was a saying in amazing Peter Bourgon blog post "A theory of Modern Go": magic is bad; global state is magic.

This is very true also in case of Prometheus client, especially if you are instrumenting some library with metrics
that your project, or maybe anyone in open source is using. This Prometheus library especially Go one,
allows you to use globals for certain simplicity, however the usage of it leaked as a pattern which we really really want to obsolete.

So let's focus on where you can have magic in our metrics and what can go wrong, on example of our loadbalancer service.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #1: Global Registry" subtitle="We have 2 global variables here."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```diff 3,4,5,6,7,8 title="Pitfall #1: Global Registry" subtitle="Package-level metric variable."
```

```go 12[16:80] title="Pitfall #1: Global Registry" subtitle="and global DefaultRegisterer."

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Global Registry" subtitle="What if another package will create metric with same name?"

// Inside package A:
var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.MustRegister(serverRequestsTotal)
}

// Somewhere inside package X imported by your dependency:
func init() {
    // PANIC!
    // You cannot register same metric name twice.
    prometheus.MustRegister(prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Tracks the number of different HTTP requests.",
        }, []string{"code", "method"},
    ))
}

// ...
// Increment our counter with written status code and request method.
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go 15,16,17,18,19,20,21,22 title="Pitfall #1: Globals: No flexibility" subtitle="What if I have more handlers than one?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Globals: No flexibility" subtitle="Nice, but for what handler?"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="get"} 2445
http_requests_total{code="500", method="get"} 53

```

```go title="Pitfall #1: Getting rid of globals"

var (
	serverRequestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Tracks the number of HTTP requests.",
		}, []string{"code", "method"},
	)
)

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Introduce instance of metrics!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

func init() {
	prometheus.DefaultRegisterer.MustRegister(serverRequestsTotal)
}

// For endpoint /one:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
serverRequestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Create new instance of ServerMetrics."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics() *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	return m
}

metrics := NewServerMetrics()

func init() {
	prometheus.DefaultRegisterer.MustRegister(metrics.requestsTotal)
}

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="Register using Custom Registerer (composition!)"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()
metrics := NewServerMetrics(reg)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

``` go title="Pitfall #1: Getting rid of globals" subtitle="Is it ok now?"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

// This will panic, because we register same metric 3 times...
metrics1 := NewServerMetrics(reg)
metrics2 := NewServerMetrics(reg)
metrics3 := NewServerMetrics(reg)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go 23,26,29 title="Pitfall #1: Getting rid of globals" subtitle="We can wrap register to inject label."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
)
metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
)
metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
)

// For endpoint /one:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

```

```go title="Pitfall #1: Getting rid of globals" subtitle="We can have request counter per handler (:"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

reg := prometheus.NewRegistry()

metrics1 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/one"}, reg),
)
metrics2 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/two"}, reg),
)
metrics3 := NewServerMetrics(
    prometheus.WrapWithLabels(prometheus.Labels{"handler":"/three"}, reg),
)

// For endpoint /one:
metrics1.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /two:
metrics2.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

// For endpoint /three:
metrics3.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{handler="/one", code="200", method="get"} 1445
http_requests_total{handler="/one", code="500", method="get"} 23
http_requests_total{handler="/two", code="200", method="get"} 445
http_requests_total{handler="/two", code="500", method="get"} 0
http_requests_total{handler="/three", code="200", method="get"} 645
http_requests_total{handler="/three", code="500", method="get"} 40

```

</CodeSurfer>

<Notes>

1) Let's take our example or http requests_total metric. We have 2 global states here.

2) As you can see metric is a global, package level variable. We register it once per package import as well.

3) Second place of global state is MustRegister which is actually hiding a Global DefaultRegisterer, so
we are registering out metric in global state of Prometheus library.

Now what's the issue here? Why is that problematic?

4) First problem is well magic.. If another package you just import, or you dependency imports registers a metrics
with the same name your application will crash at start.
Even worse the stacktrace will give you the clue where the second register happened but nothing about first one!
We have seen a lot of those problems, so please don't use globals (:

5) Second issue is lack of flexibility! What if you have more than one handler, more than one endpoint?

6) As you can see the insight you gain is pretty limited as the requests are counted per all endpoints. I can't tell
what's the error rate for /three endpoint for example.

Let's try to fix this!

7) And by fixing I mean removing globals!

8) Let's replace global variable with some struct that you can instantiate. It will have constructor that will create
our counter.

9) Now you can create such object and use it everywhere.

10) To eliminate last global behind registering, let's inject custom register into our constructor and instantiate
our own registry which we control in explicit way during our application lifetime!

11) Now what if we try to have different metrics for each handler? It will panic again, but there is nice solution
to that and since we are in control let's apply it.

12) Prometheus allows wrapping registry with custom prefix or labels, so we can inject handler label for each endpoint.

13) Now we have our metrics nicely grouped by handler as well, so we have additional crucial insight on what's
the number of requests per endpoint as well.

</Notes>

---

import testing from './static/testing.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #2: No Tests For Metrics</h2>
<br/>
<br/>
<img src={testing} style="height: 50%; margin-top: 5%"/>

<Notes>

This is something I am really passionated about.
Metrics and other observability signals like e.g tracing, logs and profiles are rarely tested.

Who is asserting in unit test if log or trace was produced for certain even and if it has certain message.
It's not always true but logs are usually used by humans so there is no point in checking exact message.

With metrics in my opinion it's totally different story. They has to be tested. Let me explain in second why.

</Notes>

---

import d12 from './static/d12.png'

<h2 style="text-align: center; margin: 0 10px 0 10px;"> Let's test our LB!</h2>

<Image src={d11} size='contain'/>

<Notes>

So let's take our loadbalancer, again and newly added HTTP request total counter.
We are solid 10x developers right, so we want to test our code properly, so we wrote
some unit test!

</Notes>

---

import d13 from './static/d13.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Unit testing lbtransport.RoundTripper</h2>

<Image src={d13} size='contain'/>

<Notes>

So the test goes like this..

We mock the Discoverer and Round Robin Picker.
We mock our Replicas for certain test case, for example something simple: all replicas, all targets
are down and not available.

Last but not the lease we will be sending few mocked HTTP requests against that.

</Notes>

---

import d14 from './static/d14.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Unit testing lbtransport.RoundTripper</h2>

<Image src={d14} size='contain'/>

<Notes>

So we run out test, we run 3 request, we assert that response is  so Bad Gateway. This is what we
expect as a response from loadbalancer as no underlying backend can serve the response.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="How it looks in Go code?"

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
}

```

```go title="Pitfall #2: Not testing" subtitle="Send 3x HTTP Requests"

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

}

```

```go title="Pitfall #2: Not testing" subtitle="Assert 3x responses with 502 status code."

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

}

```

</CodeSurfer>

<Notes>

1) Ok, so let's see how it looks in our Go code!
2) We then create a unit test,
3) Send 3 HTTP requests,
3) Assert 3x responses 502 status code.. and all good!

We are good, we can test different cases in similar way, everything passed and we are fine, right?

</Notes>

---

import d15 from './static/d15.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Do we verified everything?</h2>

<Image src={d15} size='contain'/>

<Notes>

Nice, but what if did not instrument our metric correctly, right?

For example we made a bug while incrementing a http request total metric, and we always
instrument it with 200 HTTP request code.

Now what you can see.. our unit test passed just fine, however the actual metric exposed
to Prometheus shows incorrect information.

You can think that this is not a big deal, just some analytic guys will be a little bit mislad.
Well it can be actually, very dangerous bug.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="Alert for too many errors will be missed!"

alert: TooManyHTTP502Errors
  expr: |
    sum(rate(http_requests_total{code="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with code 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

</CodeSurfer>

<Notes>

Imagine very popular alert like this: TooManyHTTP502Errors. We expect this to be triggered, and notify us, maybe
even during the night, we users sees lot's of errors from our loadalancer. It's very common symptom alert.

Now with out bug, our metric has incorrect code, so if the incident happens we are not notified and alerting is suddenly
unreliable. This is really bad situation.

</Notes>

---

import d16 from './static/d16.png'

<h2 style="text-align: center; margin: 0px 10px 0 10px;">Let's verify correct metric instrumentation!</h2>

<Image src={d16} size='contain'/>

<Notes>

So let's extend out test, with few extra steps we can verify more - so check if we instrumented out metric well!

So first let's assert cardinality is 0, so nothing was increment if no traffic was there - that makes sense
Do requests, assert respone code, and then also check cardinality. We expect only single metric 502
Now we check the value of this metric. Since we made 3 requests and all failed with 502, we expect number 3.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric value."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

}

```

```go title="Pitfall #2: Not testing" subtitle="Let's verify correct metric instrumentation!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Assert http_requests_total{code="502", method="get"} == 3 .
    testutil.Equals(t, 3,
        promtestutil.ToFloat64(metrics.requestsTotal.WithLabelValues("502", "get")),
     )
}

```

```go title="Pitfall #2: Not testing" subtitle="Such unit test will detect the problem!"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

import (
    promtestutils "github.com/prometheus/client_golang/prometheus/testutil"
)

func TestLoadbalancer(t *testing.T) {
    lb := NewLoadbalancingTransport(..., notAvailableBackends)
‚Äã
    // Assert 0 cardinality for http_requests_total{}
    // No requests, no metric should be exposed.
	testutil.Equals(t, 0, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Send 3x HTTP requests.
    rec1 := httptest.NewRecorder()
    lb.ServeHTTP(rec1, httptest.NewRequest("GET", "http://mocked", nil))
    rec2 := httptest.NewRecorder()
    lb.ServeHTTP(rec2, httptest.NewRequest("GET", "http://mocked", nil))
    rec3 := httptest.NewRecorder()
    lb.ServeHTTP(rec3, httptest.NewRequest("GET", "http://mocked", nil))

    // Assert 3x responses with 502 status code.
    testutil.Equals(t, 502, rec1.Code)
    testutil.Equals(t, 502, rec2.Code)
    testutil.Equals(t, 502, rec3.Code)

    // Assert 1 cardinality for http_requests_total{} .
    testutil.Equals(t, 1, promtestutil.CollectAndCount(lb.metrics.requestsTotal))

    // Assert http_requests_total{code="502", method="get"} == 3 .
    // === RUN   TestLoadbalancer/#00
    //       --- FAIL: TestLoadbalancer/#00 (0.00s)
    //           transport_test.go:190:
    //
    //               exp: 3
    //
    //               got: 0
    //   FAIL
    //
    testutil.Equals(t, 3,
        promtestutil.ToFloat64(metrics.requestsTotal.WithLabelValues("502", "get")),
     )
}

```

</CodeSurfer>

<Notes>

1) And let's do the same in the code! Let's get beck to our Server Metric struct with http_requests_total
2) The we had our test as it was.
3) Let's add handy promtestutil package and set our first assertion, 0 cardinality of our metric being 0, thanks
to CollectAndCount
4) After we performed requests we are asserting cardinality 1
5) And then we make sure our metric has value 3 for 502 code
6) Thanks for of if we would run this test we can detect our bug super quickly. Avoiding surprises.

As you hopefully see this is really important.
We really recommend to extend your normal tests with metrics assertion for all your crucial metrics.

Now I will let Kemal to talk about further Pitfals:
</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #3: Lack of Consistency</h2>
<br/>
<br/>
<Appear>

<h5 style="text-align: center; margin: 0 10px 0 10px;">The Four Golden Signals, USE and RED methods</h5>

<div>
<br/>
<br/>
<ul>
<li><span style="color: red">R</span>: Requests per second (saturation).</li>
<li><span style="color: red">E</span>: Errors per second.</li>
<li><span style="color: red">D</span>: Duration (tail latency).</li>
</ul>
</div>
</Appear>

<Notes>

There are some useful high-level methods on what metrics you should define TO *properly monitor your ONLINE system*.

1. The four golden signals from SRE book, USE method, RED method.

	There are certain advantages to follow them:

	- They help to ensure you have main signals upfront, FOR potential debugging or alerting needs.

	- They help you to create a common ground to communicate with others.

	- They help to reuse common alerts, recording rules and dashboards. For example, monitoring-mixins.

2. Let's focus on RED method as an example: R stands for request/sec, E for error, D for the duration.

</Notes>

---

<CodeSurfer>

```go title="Pitfall #3: Lack of consistency" subtitle="Does this satisfy RED method?"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```diff 11,13 title="Pitfall #3: Lack of consistency" subtitle="R = Requests"
```

```diff 13[25:30] title="Pitfall #3: Lack of consistency" subtitle='E = Errors (code = "5..")'
```

```diff 10[1:2] title="Pitfall #3: Lack of consistency" subtitle="D = Duration is Missing!"
```

```go 4,16,17,18,19,20,21,22,23,24,31,32 title="Pitfall #3: Consistency" subtitle="D = Histogram for observing latency"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
        requestDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "http_request_duration_seconds",
                Help:    "Tracks the latencies for HTTP requests.",
                Buckets: []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 60, 90, 120},
            },
            []string{"code", "method"},
        ),
	}
	reg.MustRegister(m.requestsTotal, m.requestDuration)
	return ins
}

// Top level ServeHTTP handler.
func ServeHTTP(w http.ResponseWriter, r *http.Request) {
	start := time.Now()
	defer metrics.requestDuration.WithLabelValues(statusRec.Status(), r.Method)).Observe(time.Since(start))

	statusRec := newStatusRecorder(w)
	next.ServeHTTP(statusRec, r)

	// Increment our counter with written status code and request method.
	metrics.requestsTotal.WithLabelValues(statusRec.Status(), r.Method)).Add(1)
}

```

</CodeSurfer>

<Notes>

Let's see an example. We have an `http_requests_total` metric to track the number of HTTP requests.

Does our Server Metrics satisfy RED method?

1. It certainly counts number of requests per method and status code.

1. We can extract `Errors` easily by checking status code.

1. But we don't have anything to track request duration!

1. By adding a histogram that observes latency of the requests, we can satisfy RED method.

Now our loadbalancer has consistent metrics and can reuse RED dashboards, alerts and recording rules, which
is just awesome.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #4: Naming - Not conforming naming convention</h2>

<Notes>

**Of course, one of the most common pitfall is about naming**

...

</Notes>

---

> There are only two hard things in Computer Science: _cache invalidation_ and _naming things_.

> Phil Karlton

<Notes>

Because, *you know*, naming things is hard!

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">There is an official documentation on naming conventions</h2>

<div style="font-size: 80%; text-align: center">
	<h6><a>https://prometheus.io/docs/practices/naming/#metric-and-label-naming</a></h6>
</div>

<Notes>

But lucky for us, we have an official documentation, FOR naming `metrics`.

**Use it!** It's really helpful.

</Notes>

---

<Notes>

A couple of points to highlight from the official documentation.

1. Names should have a suffix describing the **base** unit in plural form.

1. An _accumulating_ counter has to have *_total* as a suffix, in addition to the unit.

Actually, with new **OpenMetrics** standards this will become mandatory.

1. Put *_info* suffix at the end, for the metrics that provides metadata about your running system.

</Notes>

<CodeSurfer>

```go title="Pitfall #4: Naming - Not conforming naming convention"
```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="Names should have a suffix describing the unit."

http_request_duration_seconds
node_memory_usage_bytes

```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="Counter names have *_total* as a suffix."

http_request_duration_seconds
node_memory_usage_bytes

http_requests_total
process_cpu_seconds_total

```

```go title="Pitfall #4: Naming - Not conforming naming convention" subtitle="_info suffix for metadata metric names."

http_request_duration_seconds
node_memory_usage_bytes

http_requests_total
process_cpu_seconds_total

build_info

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #4: Naming - Stability</h2>

<Notes>

There is one other important aspect of naming: **it's stability**.

</Notes>

---

<Notes>

Let's use the same metric from previous examples, the one that tracks HTTP requests.

1. Let's build an alert using it.

1. At a certain point in time, for whatever reason, you decide to change your metric and you add *_protocol_* in the name.

1. **NOW** This alert will never fire but also WON'T fail!
**renaming** can cause issues like this, in Alerts, Recording rules, Dashboards and possibly more...

So be consistent, be careful with the names.

</Notes>

<CodeSurfer>

```go title="Pitfall #4: Naming - Stability"

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="Pitfall #4: Naming - Stability" subtitle="Let's say we alert on too many 5xx responses."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: TooManyHTTP502Errors
  expr: |
    sum(rate(http_requests_total{code="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with code 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go 11[29:37] title="Pitfall #4: Naming - Stability" subtitle="Let's say we are renaming metric..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

alert: TooManyHTTP502Errors
  expr: |
    sum(rate(http_requests_total{code="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with code 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

```go title="Pitfall #4: Naming - Stability" subtitle="Ooops..."

type ServerMetrics struct {
	requestsTotal    *prometheus.CounterVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_protocol_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

## BOOM!üí• This alert will never fire but also will not fail!
alert: TooManyHTTP502Errors
  expr: |
    sum(rate(http_requests_total{code="502"}[1m])) /
      sum(rate(http_requests_total[1m])) * 100 > 5
  for: 5m
  labels:
    severity: error
  annotations:
    summary: "HTTP errors 502 (instance {{ $labels.instance }})"
    description: |
      "Too many HTTP requests with code 502 (> 5%)\n  VALUE = {{ $value }}\n
      LABELS: {{ $labels }}"

```

</CodeSurfer>

---
import harry_potter from './static/harry_potter.gif'

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #5: Cardinality - Unbounded labels</h2>

<img src={harry_potter} style="height: 50%; width: 80%; margin-top: 5%"/>

<Notes>

When we talk about Prometheus and **performance**, it always comes down to cardinality.

What is cardinality actually?

In Prometheus context, the cardinality is the amount of unique time-series you have in your system.

And don't forget, each unique label value, that you add to your metric, creates a new time-series.

`Labels what make Prometheus strong`.

However you should always watch out how you use them.

Things could get out of control pretty quickly.

Let's see an example.

</Notes>

---

<Notes>

Let's use our usual suspect again.

1. But this time, let's add an additional label to track each requests per *path*.

1. Let's check out our metrics...
It looks reasonable, we just have a couple of addition series.

1. Let's have a closer look! *OOPS*. That is not what we expected.

We have a lot of random request from internet, which ends up in our metrics.

We shouldn't use arbitrary data as label values, such as the dynamic paths, session ids, request ids.

So, **always keep track of things that you put in your labels.**

If you want to track your discreet events, better to use a logging system.

</Notes>

<CodeSurfer>

```go title="Pitfall #5: Cardinality - Unbounded labels" subtitle="Let's define a metric." 5:9

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="#5: Cardinality - Unbounded labels"  subtitle="Let's track request per path"

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	m := &ServerMetrics{
		requestsTotal: prometheus.NewCounterVec(
			prometheus.CounterOpts{
				Name: "http_requests_total",
				Help: "Tracks the number of HTTP requests.",
			}, []string{"code", "method", "path"},
		),
	}
	reg.MustRegister(m.requestsTotal)
	return ins
}

```

```go title="#5: Cardinality - Unbounded labels" subtitle="Let's check out our metrics..."

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="GET", path="/metrics"} 15
http_requests_total{code="200", method="GET", path="/status"} 2
http_requests_total{code="200", method="GET", path="/ping"} 123
http_requests_total{code="200", method="GET", path="/articles"} 221
http_requests_total{code="200", method="GET", path="/article/1"} 1
http_requests_total{code="200", method="GET", path="/article/2"} 14
http_requests_total{code="200", method="GET", path="/article/3"} 10

```

```go title="#5: Cardinality - Unbounded labels" subtitle="Let's look closer üò≤"

# HELP http_requests_total Tracks the number of HTTP requests.
# TYPE http_requests_total counter
http_requests_total{code="200", method="GET", path="/metrics"} 15
http_requests_total{code="200", method="GET", path="/status"} 2
http_requests_total{code="200", method="GET", path="/ping"} 123
http_requests_total{code="200", method="GET", path="/articles"} 221
http_requests_total{code="200", method="GET", path="/article/1"} 1
http_requests_total{code="200", method="GET", path="/article/2"} 14
http_requests_total{code="200", method="GET", path="/article/3"} 10
http_requests_total{code="401", method="GET", path="/articles"} 221
http_requests_total{code="401", method="GET", path="/article/1"} 1
http_requests_total{code="401", method="GET", path="/article/2"} 14
http_requests_total{code="401", method="GET", path="/article/3"} 10
http_requests_total{code="401", method="GET", path="/article/4"} 1
http_requests_total{code="403", method="GET", path="admin"} 287
http_requests_total{code="404", method="GET", path="/article/112"} 6
http_requests_total{code="404", method="GET", path="/article/222"} 48
http_requests_total{code="404", method="GET", path="/article/hello"} 10
http_requests_total{code="404", method="GET", path="/article/99"} 1
http_requests_total{code="404", method="GET", path="/robots.txt"} 11424
http_requests_total{code="404", method="GET", path="/lookup/zzx"} 12
http_requests_total{code="404", method="GET", path="/helloissomeonethere"} 1

```

</CodeSurfer>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #6: Cardinality - Histogram Explosion</h2>

<Notes>

Histograms are more complex metric types compare to counters and gauges.

They do sampled observations, and we typically use them on request durations or response sizes,
as we've already seen on previous examples.

But they are more difficult to use **correctly**.

SO, **what's the problem with cardinality here?**

Underneath, a single histogram includes a couple of counters with labels.
A counter per bucket, plus *sum* and *count* counters for all observed values.

By default an  histogram exposes 10 buckets, that means 12 counters in total.

Let's see this on an example.

</Notes>

---

<Notes>

Again let's start with adding our good old http request duration metric.
Let's say we ran our loadbalancer for a while.

1. Let's check our metrics. *It works*.

	`But there's something slightly off here`, on average requests take longer than we anticipated.

	We don't have enough visibility for the requests that take longer.

1. So let's add some more metrics, to get more granular picture.

	It's fine just a couple of more time series per histogram.

1. Now, it looks better. Let our system run for awhile.

</Notes>

<CodeSurfer>

```go title="Pitfall #6: Histogram Cardinality Explosion"

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="Let's check our metrics"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 8
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 302
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 3230.08239999999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 302

```

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="We need more buckets" 3

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3, 6, 9, 20, 30, 40, 50, 60, 90, 120, 150, 200}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #6: Histogram Cardinality Explosion"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 6
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 8
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="6"} 30
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="9"} 33
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="20"} 38
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="30"} 39
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="60"} 41
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="90"} 45
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="120"} 47
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="150"} 118
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="200"} 258
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 302
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 3230.08239999999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 302

```

</CodeSurfer>

---

import itisfine from './static/itisfine.jpg'

<img src={itisfine} style="width: 90%; margin-top: 3%"/>

<Notes>

**It's fine, we got this covered!**

Right?

At this point,

**We got an alert that warns us against increased memory consumption** in our monitoring system,

and we checked our metrics to see what's happening.

We see this! ->

</Notes>

---

<CodeSurfer>

```go title="Pitfall #6: Histogram Cardinality Explosion" subtitle="BOOM!üí• Anything you add builds up."

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.01"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="0.6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="1"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="3"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="6"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="9"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="20"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="30"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="40"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="50"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="60"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="90"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="120"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="150"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="200"} 2
http_request_duration_seconds_bucket{code="200",handler="/lb",method="post",le="+Inf"} 2
http_request_duration_seconds_sum{code="200",handler="/lb",method="post"} 0.0026940999999999996
http_request_duration_seconds_count{code="200",handler="/lb",method="post"} 2
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.001"} 0
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.01"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="0.6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="1"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="3"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="6"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="9"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="20"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="30"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="40"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="50"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="60"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="90"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="120"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="150"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="200"} 146
http_request_duration_seconds_bucket{code="200",handler="/metrics",method="get",le="+Inf"} 146
http_request_duration_seconds_sum{code="200",handler="/metrics",method="get"} 0.3082099000000001
http_request_duration_seconds_count{code="200",handler="/metrics",method="get"} 146
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-500-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-500-sometimes",method="post"} 4.01e-05
http_request_duration_seconds_count{code="200",handler="demo-500-sometimes",method="post"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.01"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="0.6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="1"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="3"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="6"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="9"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="20"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="30"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="40"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="50"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="60"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="90"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="120"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="150"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="200"} 1
http_request_duration_seconds_bucket{code="200",handler="demo-refused-conn-sometimes",method="post",le="+Inf"} 1
http_request_duration_seconds_sum{code="200",handler="demo-refused-conn-sometimes",method="post"} 4.46e-05
http_request_duration_seconds_count{code="200",handler="demo-refused-conn-sometimes",method="post"} 1

```

</CodeSurfer>

<Notes>

With histograms, cardinality multiplies very quickly!

Be very considered with your histograms.

Choose what you put in your labels wisely, especially when you are using histograms!

**And do not just choose your histogram buckets randomly.**

Which is actually gets me to my next point.

</Notes>

---

<h2 style="text-align: center; margin: 0 10px 0 10px;">Pitfall #7: Poorly Chosen Histogram Buckets</h2>

<Notes>

Histograms are great to collect observations from your system.

They can be used to produce **arbitrary quantile estimations**!

Because they accumulate their observation into buckets,

they have significantly **fewer storage requirements** compare to storing raw data.

As a result performance and the **accuracy** of your histograms tightly depend on your bucket layout.

Let's see some examples.

</Notes>

---

<Notes>

Again let's start with adding our good old http request duration metric.

1. Let's create an arbitrary bucket layout that we think could work.

1. Let's check our metrics. So what's wrong here.

	Prometheus implements histograms as **cumulative histograms**.

	*WHICH means each bucket is a counter of observations, less than or equal to your specified boundary*.
	Each bucket contains the counts of all prior buckets.

	As you see in this example, we are observing wrong interval for request durations.

1. Let's fix it.

	By using one of the convenience methods from Prometheus client library.

1. Looks better.

	So coming up with a good bucket layout is hard. You need to **know your distribution**.

	**Accuracy** is controlled by the granularity of your bucket layout.

	*However*, On the other hand, adding more bucket **increases cardinality by magnitudes**.

	Which leads performance problems.

My suggestion define your service level objectives and create your buckets accordingly.

**For example**, set the `highest bucket` to the **value that violates your SLAs**,

and don't care about the rest of the high latency requests.

</Notes>

<CodeSurfer>

```go title="Pitfall #7: Poorly chosen Histogram buckets"

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Let's define our initial bucket layout" 3

var (
	buckets = []float64{0.001, 0.01, 0.1, 0.3, 0.6, 1, 3}
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="So what's wrong here?"

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 1
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.01"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.1"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.3"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.6"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="1"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="3"} 49
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 0.11527970000000001
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 49

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Let's fix it." 3:5

var (
	// prometheus client libraries are your friends!
	buckets = prometheus.LinearBuckets(0.001, 0.0002, 6)
	// buckets = prometheus.LinearBuckets(0.001, 2, 6)
)

type ServerMetrics struct {
	requestDuration  *prometheus.HistogramVec
}

// NewServerMetrics provides ServerMetrics.
func NewServerMetrics(reg prometheus.Registerer) *ServerMetrics {
	ins := &ServerMetrics{
		requestDuration: prometheus.NewHistogramVec(
			prometheus.HistogramOpts{
				Name:    "http_request_duration_seconds",
				Help:    "Tracks the latencies for HTTP requests.",
				Buckets: buckets,
			},
			[]string{"code", "method"},
		),
	}
	reg.MustRegister(ins.requestDuration)
	return ins
}

```

```go title="Pitfall #7: Poorly chosen Histogram buckets" subtitle="Know your distribution."

# HELP http_request_duration_seconds Tracks the latencies for HTTP requests.
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.001"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0012000000000000001"} 3
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0014000000000000002"} 10
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0016000000000000003"} 21
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0018000000000000004"} 25
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="0.0020000000000000005"} 29
http_request_duration_seconds_bucket{code="200",handler="/lb",method="get",le="+Inf"} 49
http_request_duration_seconds_sum{code="200",handler="/lb",method="get"} 0.10939249999999999
http_request_duration_seconds_count{code="200",handler="/lb",method="get"} 49

```

</CodeSurfer>

---

### In Summary

<Notes>

1. Observe your applications, *monitoring is not optional*.

	You have to know what's going on with your application on production.

	**Determine your service level objectives.**

	Build alerts on them.

	`Build dashboards on them.`

1. Now, since you have also alerts and dashboards in place.

	You rely on them.

	**They are liability.**

	Test them as you test your business logic.

1. Last but not least,

	**Avoid global state**, make your life easier, **for yourself**.

</Notes>

<Appear>
	<h2>Monitoring is essential.</h2>
	<h2>Unit Test Your Instrumentation.</h2>
	<h2>Avoid Global Registry.</h2>
</Appear>

---

import ss_repo from './static/ss_repo.png'

##### [https://github.com/observatorium/observable-demo](https://github.com/observatorium/observable-demo)

<Image src={ss_repo} size='contain' />

<Notes>

If you want to learn more, try it yourself or dig deeper in the code,

here is a link to our fully-functional loadbalancer service.

</Notes>

---

# Thank you!

import red_hat_white from './static/red_hat_white.png'

<img src={red_hat_white} style="height: 20%"/>

<div style="font-size: 70%">

##### [https://github.com/kakkoyun/are-you-testing-your-observability](https://github.com/kakkoyun/are-you-testing-your-observability)

</div>

<Notes>

I think that's it.

Thank you very much for listening.

Feel free to ask questions.

</Notes>

---

# References:

<div style="font-size: 90%">

* [Prometheus](https://prometheus.io)
* [Prometheus - client_go](https://godoc.org/github.com/prometheus/client_golang/prometheus)
* [Thanos](https://thanos.io)
* [Why globals are magic](https://peter.bourgon.org/blog/2017/06/09/theory-of-modern-go.html)
* [RED Method](https://grafana.com/blog/2018/08/02/the-red-method-how-to-instrument-your-services)
* [Prometheus - Histogram](https://prometheus.io/docs/practices/histograms/)
* [Prometheus Histograms ‚Äì Past, Present, and Future](https://www.youtube.com/watch?v=7sQFkaMCyEI)
* [Roboust Perception Blog](https://www.robustperception.io/blog)

</div>

